\documentclass[USenglish]{article}
%3,000 and 4,000 w
\ifx\directlua\undefined\ifx\XeTeXcharclass\undefined
  %\usepackage[utf8]{inputenc}                           %pdftex engine
  \else\RequirePackage[no-math]{fontspec}[2017/03/31]\fi %xetex engine
  \else\RequirePackage[no-math]{fontspec}[2017/03/31]\fi %luatex engine
\usepackage[small]{dgruyter}


%to do
% Hedvig: ridgeplots

% what UD says about multivalues (Gender=Masc,Fem): ``It is possible to declare that a feature has two or more values for a given word: Case=Acc,Dat. The interpretation is that the word may have one of these values but we cannot decide between them. Such multivalues should be used sparingly. They should not be used if the value list would cover the whole value space, or the subspace valid for the given language. That would mean that we cannot tell anything about this feature for the given word, and then it is preferable to just leave the feature out.''

% what UD says about `missing' features: ``Not mentioning a feature in the data implies the empty value, which means that the feature is either irrelevant for this part of speech, or its value cannot be determined for this word form due to language-specific reasons.'' https://universaldependencies.org/u/overview/morphology.html

% Define fonts for non-latin characters
\newfontfamily\Timesfont{Times New Roman}
\newfontfamily\dolousfont{Doulos SIL}


\usepackage{hyperref}
\hypersetup{
	unicode,
%	colorlinks,
%	breaklinks,
%	urlcolor=cyan, 
%	linkcolor=blue, 
%	pdfauthor={Author One, Author Two, Author Three},
%	pdftitle={A simple article template},
%	pdfsubject={A simple article template},
%	pdfkeywords={article, template, simple},
%	pdfproducer={LaTeX},
%	pdfcreator={pdflatex}
}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{placeins}
\usepackage{natbib}
\usepackage{subfigure}
\usepackage{longtable}


\usepackage{graphicx, color}
\graphicspath{{latex/fig/}}
\setlength{\emergencystretch}{2em}

\begin{document}

%TC:ignore

%  \articletype{...}
 \author*[1]{ANON}
%  \author*[1]{Hedvig Skirgård}
%  \author[1]{Stephen Francis Mann}
%  \runningauthor{...}
 \affil[1]{ANON}
 % \affil[1]{Department of Linguistic and Cultural Evolution, Max Planck Institute for Evolutionary Anthropology, Leipzig, Germany}
%  \affil[2]{...}
  \title{Using annotated cross-linguistic corpora to explore morphological information}
%  \runningtitle{...}
%  \subtitle{...}
  \abstract{
  The emergence of annotated corpora collections in many different languages opens up new avenues for research in linguistic typology, enabling researchers to take into account more nuances of language use than previously.
    In this paper, we use the morphological feature annotations in the Universal Dependencies datasets to quantify the information carried by morphology.
    Morphological information is one of many metrics covered by the term ``linguistic complexity''.
    We propose an approach founded in information theory which focusses on how surprising a certain morphological annotation is given a token's lemma.
    Token-level informational quantities are aggregated to a dataset-level average, which serves as a proxy for a language's morphological information load.
    We find that languages vary in how much information is encoded in morphology and that the variation correlates moderately with measurements of fusion, another metric that has been used to quantify ``linguistic complexity''.
    We argue that measurements such as the one presented in this paper make it possible to test more specific hypotheses of ``linguistic complexity''; for example, how differences in the pragmatics of common ground can impact grammar.
    We also investigate variants of the basic approach (e.g. generalising over part-of-speech instead of lemma) and discuss shortcomings (comparability of datasets and more).  
  }
  \keywords{morphology, Universal Dependencies, corpora, information theory, surprisal}
%  \classification[PACS]{...}
%  \communicated{...}
%  \dedication{...}
%  \received{...}
%  \accepted{...}
%  \journalname{...}
%  \journalyear{...}
%  \journalvolume{..}
%  \journalissue{..}
%  \startpage{1}
%  \aop
%  \DOI{...}

\maketitle

%TC:endignore
	
\newpage
\section{Introduction}
Do some languages necessarily encode more information than others?
All languages can express all concepts, albeit in different ways and at varying lengths. 
There may not be a word for the German expression ``schadenfreude'' in all languages, but the meaning can be approximated with several words or clauses. 
Grammar is the set of rules of a language that determine how to put units together, and what has to be specified: ``grammar [...] determines those aspects of each experience that must be expressed'' \citep[132]{boas1938language}. 
Languages differ in their grammar, they differ in what ``must be expressed'' and how. 
For example, some languages have pronoun systems with a distinction between inclusive and exclusive ``we'' (e.g. S\={a}moan l\={a}tou / m\={a}tou). 
While the English language does not have such a grammatical distinction in its pronoun system, the information can be expressed optionally (e.g. ``we, you and I, are going'') or can be left ambiguous without being ungrammatical.
In S\={a}moan, the distinction is made every time ``we'' is expressed, and the grammar of S\={a}moan therefore entails a higher degree of information specificity in this particular regard.
Conversely, there is other information that English grammar specifies which S\={a}moan does not, such as masculine/feminine gender on 3rd person pronouns.


 \section{Background: defining language complexity}\label{sec:background}
% \subsection{``Language complexity''}
%Measuring information

Language complexity is a highly multifaceted concept, with many theories relating its varied aspects with the social circumstances of language communities.
%%% SFM word chopping
% Language complexity has fascinated linguists and scholars of other fields such as psychology and anthropology for a long time. 
% There are many theories regarding this concept and its relation to the social circumstances of language communities. 
%%% SFM end word chopping
Research to date has primarily focussed on relating language complexity to contact languages \citep{mcwhorter_2003} and population size and/or proportion of non-native speakers (inter alia \citet{wray2007consequences, dahl2004growth, lupyan2010language, bentz2013languages, bentz2015adaptive, raviv2019larger, koplenig2019language, shcherbakova2023societies}).
Broadly, one can characterise the field of research as being concerned with how and under what conditions languages acquire more complexity than would seem `necessary' for pure information transmission, and how such complexity can be lost. 
%%% SFM word chopping
% In particular, most research is concerned with different aspects of `morphological complexity'.
%%% SFM end word chopping

Language complexity can be operationalised into many different specific measurements. 
Some of these measurements correlate and may track the same/similar phenomena \citep{bentz2016comparison, bentz2023complexity, ccoltekin2023complexity}, or not \citep{lupyan2024cautionary}.
Table \ref{tab:complex_metrics} gives an overview of several different ways of measuring language complexity. 
The table combines measurements based on typological surveys of grammatical descriptions (e.g. WALS; Grambank) with those derived from corpora, experiments, and simulation studies.

\begin{table}[h]
    \centering
    \begin{tabular}{p{3cm}p{5cm}p{3cm}p{2cm}p{2cm}}  % Adjust column alignment as needed
        \toprule
        \textbf{Metric} & \textbf{Description}  & \textbf{Selection of relevant studies } \\ 
        \midrule
       boundedness/fusion   & how much grammatical material is morphologically bound &  \cite{grambank_release}; \cite{shcherbakova2023societies}; \citet{lupyan2010language} \\ 
         \midrule
        type-token ratio (TTR) & how many unique words in relation to all words (more morphology = more unique words)  & \cite{kettunen2014can}; \citet{ccoltekin2023complexity}\\ 
               \midrule
 synthesis & morpheme to word ratio  & \citet{easterday2021syllable} \\
                     \midrule
     information      & how many grammatical distinctions a grammar makes possible & \cite{shcherbakova2023societies};  \cite{grambank_release}; \citet{lupyan2010language}; \citet{ccoltekin2023complexity}\\
\midrule
informativity/prediction accuracy/contextual predictability  & how accurately the next item can be predicted & \cite{frank2011insensitivity}; \citet{cohen2008phone}; \citet{levshina2022frequency} \\    \midrule
redundancy  & how much information is repeated & \cite{leufkens2023measuring}\\    \midrule
systematicity/ transparency  & the degree of consistency of form to meaning patterns & \cite{raviv2019larger}; \cite{hengeveld2018transparent} \\    \midrule
regularity & the consistency of rules, for example in paradigms &  \cite{round2022cognition}‚ \cite{ackerman2009parts}; \cite{cotterell2019complexity} \\    \midrule
compositionality  & the extent to which the meaning of larger units is built from the meanings of smaller parts &  \cite{wray2007consequences} \\    \midrule
Kolmogorov complexity  & how much a computer algorithm can compress language material &  \cite{juola1998measuring,ehret2016informationtheoretic,moscosodelpradomartin2004putting,sagot2013comparing} \\    \midrule
ease of LLM-learning  & how easy is it for a large language model (LLM) to learn the language &  \cite{koplenig2023languages}  \\    \midrule
length of syntactic dependencies  &  how nested the syntactical structure is (tax on short-term memory) & \cite{gibson1998linguistic}; \citet{liu2008dependency} \\    
\midrule
shared types & the total number of population-wide shared types &  \citet{spike2017population}\\
\bottomrule
    \end{tabular}
    \caption{Non-exhaustive table of different language complexity metrics found in the research literature.}
    \label{tab:complex_metrics}
\end{table}

Depending on the particular research question being evaluated, it is desirable to specify what aspect of `language complexity' is being evaluated and how it is operationalised into specific measurement(s). %(``does a large proportion of second language learners reduce complexity?'', ``is language complexity primarily shaped by production or perception?'', ``do more modular networks create more complexity?'' etc), it is desirable to specify what facet of ``language complexity'' is being evaluated and how it is operationalised into specific measurement(s).
 %\begin{enumerate}
 %    \item More morphology -> more complexity (citations needed)
 %    \item It is desirable to measure complexity via measuring morphology
 %    \item What we want to measure is better measured by studying corpora rather than rules
 %    \item Because of (2) we want to use something mathematical and (3) we use corpus
 %\end{enumerate}

A language with a rich morphology often exhibits more unique words compared to a language with less grammatical marking overall, or grammatical marking that is free-standing.
Morphological marking is also not necessarily compositional: paradigms can include suppletion, vowel harmony and other non-compositional patterns.
In this sense, languages with more unique word forms and richer paradigmatic patterns to learn and process instantiate one dimension of language complexity (c.f. \citet{baerman2015understanding}, \citet{bentz2016comparison}, \citet{dahl2004growth}).
Therefore, one way to measure a facet of a language's complexity is to quantify the amount of morphology it contains.
Many studies have contributed to this discussion by studying the grammatical rules of different languages (cf. \citet{lupyan2010language}, \citet{shcherbakova2023societies}).
However, a language can have a rich morphological paradigm but when one studies usage it turns out several of the forms are rarely uttered.
If all forms are not equally used, that paints a different picture of the morphology of the language compared to if we only study the possibilities and implicitly treat them as equally present.
For example, in Spanish there are three distinctions in the demonstrative system (\emph{este}/\emph{ese}/\emph{acquel} \citet[87]{butt_2019_spanish}\footnote{This is the masculine singular series. The markers differ further for feminine, neuter and plural.}). 
However, out of these the first two are used the most. 
In the UD-dataset Spanish-AnCora \citep{aule2008ancora}, \emph{acquel} is used 5\% of the time, \emph{ese} 26\% and \emph{este} 69\%,.
We find a somewhat similar situation in Turkish, where there are also three distinctions in demonstratives (\emph{bu}, \emph{şu}, \emph{o} \citet[67]{turkish_2020}\footnote{This is the nominative series. The forms differ for further cases.}). 
In the UD-dataset Turkish-Penn, \emph{bu} accounts for 85\% of occurrences, \emph{o} 9\% and \emph{şu} 6\%.
Spanish and Turkish can both be said to have three distinctions in their demonstrative paradigms, but usage frequencies differ between the two languages.
This may mean that the implications for learning and processing also differ.
Corpora have the advantage of providing information about the frequency of different features in actual usage (cf. \citet{levshina_gradient}, \citet{bentz2016comparison} Saussure's \textit{langue} and \textit{parôle} \citet{Saussure1916}) as opposed to the possibilities of the system.

In this study, we focus on morphological distinctions and their distribution in corpora. 


%We can compare languages in terms of how much information is specified obligatorily in grammar. 
%In this paper we use cross-linguistic corpora to quantify this kind of morphological information and compare values across languages.
%%% SFM word chopping
% For example, it is possible that a particular grammatical distinction exists in a language such that there are two alternatives, but 90\% of the time speakers just use one alternative. 
% In an information-theoretic sense, such a distinction is less informative than one with balanced frequencies: it is easier to predict that a high-frequency alternative will occur, so its occurrence is less informative.
%%% SFM end word chopping
Informational distinctiveness in grammar -- both in rules and usage -- is one of the phenomena that can be described as measuring ``language complexity''.
Before presenting our approach, we give a brief overview of research on ``language complexity'' and where this study is situated in this landscape.




The present study lays the ground for a wider investigation of how systematic pragmatic habits, over time and over language communities, can impact the structure of the language.
%%% SFM word chopping
% In particular, we are interested in informational distinctiveness, and the other side of the coin - ambiguity.
%%% SFM end word chopping
%Several studies have sought to link sociocultural and demographic features with the various facets of linguistic complexity \citep{wray2007consequences,lupyan2010language,shcherbakova2023societies,dale_lupyan_2012}.
%We are interested in exploring how systematic corpus-based cross-linguistic comparison of morphology can contribute to testing the strength of hypotheses relating to pragmatics of common ground \citep{hall1976beyond,cardon2008critique,bentz2015adaptive, meyer2016culture, levinson2024dark}.
To that end, in this paper we define metrics of informational distinctiveness of morphological features based on datasets of the Universal Dependencies project \citep{UD_2.14}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%% SFM WORD CHOPPING %%%%%%%%%

% Is it the case that communities that tend to share a lot of common ground tend to create less informative utterances because the grammar of their languages makes fewer demands on informational distinctiveness?
% Scholars such as \citet{wray2007consequences} have suggested that communities with a lot of shared common ground develop less complex - specifically less compositional - languages due to constraints related to acquisition, conversational efficiency and processing. 
% This proposal has been tested in studies like \citet{lupyan2010language} and \citet{shcherbakova2023societies}, with contrasting results (possibly due to different samples, operationalisation of ``complexity'' and/or control for spatial and phylogenetic autocorrelation).
% We seek to explore the effect of pragmatic constraints on grammar, but contrary to \citet{wray2007consequences} consider informational distinctiveness instead of compositionality. 
% American anthropologist Edward Hall's has suggested that cultures of the world can be meaningfully understood on a dimension of ``high'' to ``low''-context communication styles \citep{hall1976beyond}.
% The high-context communicational style involves assuming a large degree of shared ground and context between interlocutors, whereas low-context involves presupposing  little shared information. 
% These pragmatic premises have consequences for the linguistic message. \citep[79]{hall1976beyond} writes: 
% \begin{quote}
% A high-context (HC) communication or message is one in which most of the information is either in the physical context or internalized in the person, while very little is in the coded, explicit, transmitted part of the message.
% A low-context (LC) communication is just the opposite; i.e., the mass of the information is vested in the explicit code.
% \end{quote}
% It is possible that communities that use HC messages more often end up with not only overall less information in their utterances (cf. ``leveraging context'' in \citet[29-31]{levinson2024dark}) - but perhaps also less information specified in the grammar of their languages.
% The second prediction follows on from the Linguistic Niche Hypothesis framework (LNH) \citep{dale_lupyan_2012} which proposes that the social context of learning and usage of a language may impact its structure.
% Hall's theory also connects the predominant communication style of a culture to the social heterogeneity, which can be compared to the connection between proportion of strangers/second-language speakers and various facets of ``complexity'' explored in the linguistics literature.
% Hall's theory of HC and LC communication has been widely applied and popular in Intercultural Business and Technical Communication \citep{meyer2016culture}.
% However, it has also been critiqued for not being empirically supported \citep{cardon2008critique}.
% We are interested in exploring how systematic corpus based cross-linguistic comparison can contribute to testing the strength of Hall's framework of cultural differences in communication styles as interpreted via LNH.
% To that end, in this paper we define metrics of informational distinctiveness of grammar (specifically morphological features) based on datasets of the UD project. 

%%%%%%% END SFM WORD CHOPPING %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\citet{hall1976beyond}
% We are interested in variation in informational load of morphology as found in language use. 
% It is well-known that languages vary with regards to morphemes being phonologically bound to each other and how they form extensive paradigms. 
% We are interested not only in the existence of these informational structures but how they are utilised. 
% To that end, we investigate the information-theoretic measure of \textbf{surprisal} of morphological features in the cross-linguistic Universal Dependencies database \citep{UD_2.14}. 
% We are interested in exploring the connections between this topic and theories proposed by \citet{wray2007consequences, lupyan2010language} and \citet{hall1976beyond}. 
% However, for the study at hand we focus on the measurements themselves and comparisons with related measurements. 
% We have also included comparisons with population size of languages, but this is merely to give indication of the context of the different measurements. 
% For a thorough examination of the relationships between these metrics of `language complexity' and social conditions, more extensive analysis is necessary.

%In this paper, we are primarily interested in the pragmatics of information transfer between adult native speakers and how the trade-off between the encoding and decoding cost is balanced in different languages. Specifically, we are interested in how languages vary with respect to morphological information.
\FloatBarrier


\FloatBarrier
\section{Study outline}
We aim to quantify the information in morphology annotations in corpora as a way of assessing how much information languages typically encode.
An increase in morphological information can be understood as increasing the difficulty of production for the sender, and thereby increasing one facet of `language complexity'.
We therefore aim to capture the amount of morphological information a language encodes in its words, on average.
Our metric, described in section \ref{detailed_procedure} and defined precisely in Appendix \ref{sec:appendix_ours}, captures the amount of information carried by morphological features across approximately 70 languages spanning 170 datasets from the Universal Dependencies corpora collection v2.14 (UD) \citep{UD_2.14}.
Annotated corpora yield a measure of morphological information per language, which serves as a proxy for speaker difficulty, which is one aspect of language complexity.

Many corpora collections such as UD contain information on the morphology of individual words. 
Table \ref{tab:turkish_example} is an example of a sentence in Turkish where words are marked for the morphological categories case, number, person, aspect and more \citep{kuzgun_2020_UD_turkish_penn}.
%For example, the word ``horses'' in English would typically be tagged for the grammatical meaning ``plural'' being encoded morphologically.

\begin{table}[h]
    \centering
    \caption{Sentence 15-0000 from Universal Dependencies dataset Turkish-Penn \citep{kuzgun_2020_UD_turkish_penn}. UPOS : Universal Part-of-Speech, lemma = base form, token = specific word, feats = morphological features} %note table captions go above the table
    \label{tab:turkish_example}   
    \begin{tabular}{p{1.5cm}p{2cm}p{2cm}p{5cm}}
\toprule
	\textbf{UPOS}	&	\textbf{lemma}	&	\textbf{token}	&	\textbf{feats}	\\
    \midrule
	ADJ	&	devasa	&	Devasa	&	\\    \midrule
	ADJ	&	ölçek	&	ölçekli	&\\    \midrule
ADJ	&	yeni	&	yeni	&		\\    \midrule
	NOUN	&	kanun	&	kanunda	&	Case=Loc|Number=Sing|Person=3	\\    \midrule
	ADJ	&	kullan	&	kullanılan	&		\\    \midrule
ADJ	&	karmaşık	&	karmaşık	&\\    \midrule
CCONJ	&	ve	&	ve	&		\\    \midrule
ADJ	&	çetrefil	&	çetrefilli	&		\\    \midrule
	NOUN	&	dil	&	dil	&	Case=Nom|Number=Sing|Person=3	\\    \midrule
	NOUN	&	kavga	&	kavgayı	&	Case=Acc|Number=Sing|Person=3	\\    \midrule
	VERB	&	bulan	&	bulandırdı	&	Aspect=Perf|Mood=Ind|Number=Sing| Person=3|Polarity=Pos|Tense=Past| VerbForm=Fin|Voice=Cau	\\\midrule
   \multicolumn{4}{p{11cm}}{Translation: \textit{The complex and complicated language in the massive new law has muddied the fight.}}\\    \bottomrule
    \end{tabular}
\end{table}

In this study, we are not only interested in how many morphological features on average a token in a given language has, but also how informative those features are given their lemma or base form. 
%If tokens with the lemma ``kanun'' almost always has the value singular for the morphological feature category number, perhaps number is not so informative for those tokens?
Words in any language can be grouped according to their lemma. 
For example ``horse'', ``horses'', ``horse's (possessive)'' all have the same lemma, usually represented in the singular, nominative non-possessive form, i.e. ``horse''.
The information stored in the morphological feature on the token can be understood as dependent on the relative frequencies of that feature on that lemma. 

For example, in the Turkish-Penn dataset of Universal Dependencies 2.14 there are 52 tokens with the lemma ``kanun'' (Eng: \textit{law}).
Of these, 47 are marked as singular for the number category, and five as plural; most of the time when people use this word in this corpus, it's singular.
Were we only to know that a token belongs to the lemma ``kanun'', we would have a good chance ($\sim$90\%) to guess correctly if we said it was in the singular form.
The category of number for this lemma is therefore not very informative, it is often the same value.
%On the other hand, if we were to see an instance of a token with the lemma ``kanun'' with plural marking, we ought to be a bit more surprised.
By contrast, tokens for the lemma ``fiyat'' (Eng: \textit{price}) have a 55\%/45\% chance of being plural vs. singular (plural = 237, singular = 196). 
Were we to only know that a token was of this lemma, we'd have a harder time guessing its value for the morphological feature number . 
The number morphology for ``fiyat'' is more informative than that for ``kanun''.
This intuition lies behind the information-theoretic measures defined in this paper.
We provide a quantitative answer to the question: how informative is the morphology of a particular language?

\subsection{Data}
We use Universal Dependencies (UD) v2.14 \citep{UD_2.14}, Grambank v1.0 \citep{grambank_release, grambank_dataset_zenodo_v1} and Glottolog v5.0 \citep{glottolog5.0}.
We also include population size data from Google's research team \citep{ritchie-etal-2024-linguameta-unified}.
We omit 20 UD datasets which lack any morphological feature annotations\footnote{The UD datasets without morphological feautre annotations are:  UD\_Akkadian-PISANDUB, UD\_Azerbaijani-TueCL, UD\_Bavarian-MaiBaam, UD\_Chinese-PatentChar, UD\_Chukchi-HSE, UD\_English-CTeTex,  UD\_English-ESLSpok, UD\_Frisian\_Dutch-Fame, UD\_Irish-TwittIrish, UD\_Korean-Kaist, UD\_Luxembourgish-LuxBank, UD\_Maltese-MUDT, UD\_Neapolitan-RB, UD\_Old\_Turkish-Clausal, UD\_Paumari-TueCL, UD\_South\_Levantine\_Arabic-MADAR, UD\_Swedish\_Sign\_Language-SSLC, UD\_Swiss\_German-UZH, UD\_Tagalog-Ugnayan and UD\_Vietnamese-VTB.}, as well as three multilingual datasets which would obscure language-specific metrics \footnote{The excluded multilingual datasets are: UD\_Turkish\_German-SAGT, UD\_Maghrebi\_Arabic\_French-Arabizi and UD\_Frisian\_Dutch-Fame.}.

\subsection{Similar studies}
% There are a few studies that describe metrics of complexity similar to our study. 
\cite{grambank_release} and \citet{shcherbakova2023societies} both use related measurements based on typological data from the Grambank dataset: Fusion (out of all Grambank features that target bound morphology, what proportion are answered positively?) and Informativity (out of all Grambank features which target if a grammatical distinction is made, how many are answered yes?\footnote{The term ``informativity'' is found in the literature with at least two senses: how many grammatical distinctions a grammar makes possible \citep{shcherbakova2023societies, grambank_release} and the average contextual unpredictability \citep{cohen2008phone}. These two senses are distinct. When we refer to the Grambank informativity metric, we refer to the first one.}$^{,}$\footnote{For more details on these metrics, please see \citet{grambank_release} and \citet{R-rgrambank}.}). These metrics differ from the study at hand in that they describe the possibilities that the grammar of a certain language contains (as found in grammatical descriptions) rather than language usage (such as can be measured in corpora). 
We include these two metrics in our results section for comparison with the measures defined here. 
We use the R-package rgrambank to derive the Grambank metrics \citep{R-rgrambank}; see appendix \ref{sec:appendix_rgrambank} for details.
%%% SFM word chopping: moved to appendix
% \footnote{The calculation of Fusion and Informativity from raw Grambank data is somewhat involved. We combine dialects into languages, and if two dialects under the same language have different values for the same feature, we choose one at random. 
% We remove languages with more than 25\% missing data across the features used to calculate each metric. 
% We do this using the functions from the package rgrambank \citep{R-rgrambank}. 
% This data wrangling procedure differs slightly from \citet{grambank_release} and \citet{shcherbakova2023societies}, but the difference is negligible. 
% See example scripts accompanying the R-package rgrambank for illustration of procedure and differences. Unlike \cite{shcherbakova2023societies}, but like \cite{grambank_release} we use the version of the Grambank metric Fusion which awards features that cover bound morphology and other morphology half a Fusion-point.} 
%% SFM end word chopping

A recent study from \citet{ccoltekin2023complexity} also uses an information theoretic approach to investigate morphological features in UD. 
Given the similarity of this study with the current study, we describe this metric in more detail in appendix \ref{sec:appendix_ccoltekin}.
Çöltekin \& Rama's goal is to determine the extent to which different measures of complexity latch onto the same underlying properties of a language. 
They analyse eight measures, from a simple count of the type/token ratio to the far more involved assessment of how accurately a machine learning model can predict an inflected word from its lemma and morphological features.
The measure most relevant for our purposes is \textbf{morphological feature entropy}, which captures how evenly spread morphological features are throughout a corpus.
This `even-spreadedness' connects to complexity in the manner outlined above: if there are many different features that have roughly similar frequency, it would be difficult to predict from a lemma alone what its features are; on the other hand if a few well-represented features dominate the corpus, a given lemma's features are likely to be more predictable.
Since predictability is intuitively connected to complexity, the even-spreadedness of morphological features in a corpus can indicate the complexity of a language.
This metric of Çöltekin \& Rama has since been used in further studies, such as \citet{bentz2023complexity}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%% SFM: Below moved to appendix %%%%%%%

% The canonical mathematical way to measure this `even-spreadedness' is entropy.
% Defined as $\Sigma p \log{\frac{1}{p}}$ for probabilities $p$ of a given distribution, entropy is large when probabilities are more evenly distributed and small when a few probabilities are much greater than the others.
% Thus entropy captures something like the average amount of uncertainty among a set of items.
% Çöltekin \& Rama thus define a measure of the entropy of all morphological features in a dataset to capture one aspect of a language's complexity.
% Their method of calculating this value is as follows:\footnote{This procedure describes the code which can be found at \url{https://github.com/coltekin/mcomplexity/blob/main/mlc-morph.py}, specifically the method \texttt{get\_mfh}.} %We are grateful to Çöltekin \& Rama for corresponding with us so that we could confirm our understanding of their procedure. Any remaining misunderstandings are our own.}:
% \begin{enumerate}
% \item For a given dataset, randomly sample a fixed number of tokens
% \item Remove punctuation (UPOS=`PUNCT') and unanalysable tokens (UPOS=`X')
% \item Remove tokens that have no morphological features
% \item Split the remaining tokens' morphological features into a list (e.g. Table \ref{tab:mfh} lists the individual features in the sample shown in Table \ref{tab:turkish_example})
% \item Count how many times each feature occurs in the sample (column `Count' in Table \ref{tab:mfh})
% \item Calculate the relative frequency of each feature (column `Frequency (p)' in Table \ref{tab:mfh}; this is the feature's Count value divided by the sum of the Count column for the entire sample)
% \item Calculate entropy from the frequencies ($\Sigma p \log{\frac{1}{p}}$)
% \item Run steps 2-7 multiple times for different samples of the same size and take the mean entropy.
% \end{enumerate}

% \noindent Table \ref{tab:mfh} shows the components of the entropy measure for the sample of tokens listed in Table \ref{tab:turkish_example}.
% The total entropy for this sample is approximately 3.15 bits.
% In their study \citet{ccoltekin2023complexity} would sample something like 1000 tokens at a time, generating an entropy measure each time and then taking the average.
% This average is then an estimate of the `overall' entropy measure for the dataset.

% \begin{table}[h]
%     \centering
%     \caption{Components of the measure of morphological feature entropy as defined by \citet{ccoltekin2023complexity} for the sample of tokens in Table \ref{tab:turkish_example}. The total feature count is 17, and each feature's Frequency is its Count divided by this total (rounded to three significant figures). The entropy of this sample is 3.15 bits (also to three significant figures).} %note table captions go above the table
%     \label{tab:mfh}   
%     \begin{tabular}{p{5cm}p{3cm}p{3cm}}
% \toprule
% 	\textbf{Morphological feature}	&	\textbf{Count}	&	\textbf{Frequency (p)}	\\
%     \midrule
% 	Case=Loc&1&0.0588       \\    \midrule
% 	Number=Sing&4&0.235    \\    \midrule
%         Person=3&4&0.235		   \\    \midrule
% 	Case=Nom&1&0.0588	       \\    \midrule
% 	Case=Acc&1&0.0588		      \\    \midrule
%         Aspect=Perf&1&0.0588      \\    \midrule
%         Mood=Ind&1&0.0588		   \\    \midrule
%         Polarity=Pos&1&0.0588		\\    \midrule
% 	Tense=Past&1&0.0588	     \\    \midrule
% 	VerbForm=Fin&1&0.0588	    \\    \midrule
% 	Voice=Cau&1&0.0588      	\\ \bottomrule

%     \end{tabular}
% \end{table}


%%%%% SFM: Above moved to appendix %%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A similar measure was previously defined by \citet{SPROAT14.47}.
Their focus is efficiency, which they interpret as packing the most information into the shortest space.
They discuss different methods of quantifying both `information' and `space' in order to obtain ratios $\frac{\text{information}}{\text{space}}$ for different languages.
One of their measures of information weights words by the \textbf{surprisal} of their morphological features.
The surprisal of an event is a function of its probability: it is the logarithm of the reciprocal, written $\log{\frac{1}{p}}$, so that events with lower probability have higher surprisal.
\citet{SPROAT14.47} obtain these probabilities by estimating them; however, they do not fully describe how the estimate is produced.
They use customised feature annotations rather than Universal Dependencies datasets, and they report results for only eight languages and fewer than 1,000 sentences per language.
Their work can therefore be seen as a step towards the kind of measure defined by \citet{ccoltekin2023complexity} as well as our own; however, their relatively small sample size suggests that quantitative comparison with our results would not be warranted.

\subsection{Overview of our metrics}
\label{detailed_procedure}

An in-depth comparison of our metric with that of \citet{ccoltekin2023complexity}, including justifications for each aspect in which our measure differs from theirs, can be found in Appendix 
\ref{sec:appendixComparison}.
Here we give an overview, describing and justifying two key differences.

\citet{ccoltekin2023complexity} use entropy as their measure.
Technically entropy is itself the average surprisal of all events in an entire distribution.
%%% SFM word chopping
% Each row in Table \ref{tab:mfh} therefore has its own surprisal, and it is the average of these that constitutes the entropy.
%%% End SFM word chopping
Our metric also uses surprisal as the basis of a measure of complexity.
However, instead of entropy, we use \textbf{mean surprisal per token}.
As noted above, calculating the morphological surprisal of a token requires calculating the probabilities of its morphological features.
Whereas \citet{ccoltekin2023complexity} calculate frequencies on a sample-by-sample basis, and \citet{SPROAT14.47} estimate frequencies in an unknown manner, we calculate frequencies across the entire corpus for each UD dataset.
The full procedure of our basic metric can be found in appendix \ref*{sec:appendix_ours}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% SFM below moved to appendix %%%

% The full procedure of our basic metric is as follows:
% \begin{enumerate}
% \item Combine the files of each UD dataset (e.g. dev, test and train) to one table with one token per row
% \item Remove punctuation (UPOS=`PUNCT'), symbols (UPOS=`SYM') and unanalysable tokens (UPOS=`X')
% \item Define lemmas as a combination of lemma + UPOS, so that lemmas that are spelled the same but belong to different UPOS can be distinguished. \textit{Example:} the noun ``mark'' and the verb ``mark'' in English are defined as having the lemmas ``mark\_NOUN'' and ``mark\_VERB'' as opposed to both potentially being assigned to the same lemma ``mark''.
% \item Determine the full set of morphological feature categories per lemma. \textit{Example:} the token `sebuah' in Table \ref{tab:unassigned_ex} has features Definite and PronType; supposing there were another token of the same lemma in the dataset that had a value for the category Number, then the full set of categories for `sebuah' would be Definite, Number and PronType.
% \item Remove features that are not related to morphology: ``Abbr'' (abbreviations), ``Typo'' (whether or not a token has a typo) and ``Foreign'' (whether or not a token is deemed as in a foreign language)
% \item Assign a dummy value to unassigned morphological categories for each token, given the other available feature categories for tokens of the same lemma. \textit{Example:} for `sebuah' we insert the dummy feature Number=unassigned.
% \item Split the tokens' morphological features into a list (e.g. Table \ref{tab:unassigned_ex_SPLIT} lists the individual features for the sample shown in Table \ref{tab:unassigned_ex})\footnote{Some tokens have been given more than one feature value for the same feature category. This is allowable within the UD-framework, though the coordinators note that such multi-values should be used sparingly. For example, the adjective ``{\dolousfont ἀ}{\Timesfont όρατος}'' in the Ancient Greek PTNK-dataset is assigned ``Gender=Fem,Masc''. We are treating instances like these as a case of the feature value being ``Fem,Masc'' and not a case of both ``Gender=Fem'' and ``Gender=Masc''.}
% \item Count how many times each feature \textit{value} occurs \textit{for that category, in tokens of that lemma} (column `Count' in Table \ref{tab:unassigned_ex_SPLIT})
% \item Calculate the relative frequency of each feature value (column `Frequency' in Table \ref{tab:unassigned_ex_SPLIT}; this is the feature value's Count divided by the number of times that lemma appears in the dataset)
% \item Calculate the surprisal of that feature value for that token: $\log{\frac{1}{\text{frequency}}}$
% \item Calculate the total `surprisal of a token' by summing the surprisals of all its features (including dummy-assigned features)

% % \item Calculate the mean surprisal in the dataset by summing the surprisals and dividing by the number of tokens.
% \end{enumerate}

% \begin{table}[h]
%     \centering
%     \caption{Four tokens from the treebank UD\_Indonesian-PUD. The token `sebuah' has no feature value for Number, but in this illustrative example we imagine that other tokens of the same lemma do have this feature. We therefore include it as an unassigned feature; likewise for `para' and the feature Definite. Of the adjectives, neither `baru' nor `terakhir' have a value for the feature NumType; we continue to imagine that there is at least one lemma for each of these tokens that does have a value for this feature, therefore both are given it as an unassigned feature. In fact the token `baru' has no feature values of its own; thus it takes all features possessed by any other token of the lemma `baru' as unassigned.} %note table captions go above the table
%     \label{tab:unassigned_ex}   
%     \begin{tabular}{p{1cm}p{1.4cm}p{1.5cm}p{3.5cm}p{2.5cm}}
% \toprule

% % UD\_Indonesian-PUD
% %id	&
% UPOS&lemma	&token	&feats & unassigned feats	\\ 
% \midrule
% DET & buah & sebuah 
% & Definite=Ind|PronType=Art
% & Number
% \\\midrule
% DET & para	& para	&Number=Plur|PronType=Ind & Definite
% \\\midrule
% ADJ&baru	&baru&
% & Degree \newline
% NumType 
% \\\midrule
% ADJ & akhir	&terakhir&	Degree=Sup& NumType\\\bottomrule
% \end{tabular}
% \end{table}

% % OUR APPROACH, FEATURES SPLIT
% \begin{table}[h]
%     \centering
%     \caption{Four tokens from the treebank UD\_Indonesian-PUD. For illustrative purposes we imagine this dataset contains 20 occurrences of the lemma `buah', 10 of `para', 5 of `baru' and 12 of `akhir'. The Count and Frequency columns, whose values in this table are also illustrative and not real, answer the question: `how often does this lemma have this value for this feature?'. The frequencies are used to calculate the surprisal of a particular token. For example, the surprisal of the token `sebuah' is $\log{\frac{1}{0.3}}+\log{\frac{1}{0.05}}+\log{\frac{1}{0.5}} = 7.06\text{ bits}$ (to three significant figures).} %note table captions go above the table
%     \label{tab:unassigned_ex_SPLIT}   
%     \begin{tabular}{p{0.7cm}p{1cm}p{1.4cm}p{1.3cm}p{1.5cm}p{1.4cm}p{1.6cm}}
% \toprule

% %id	&
% UPOS&lemma	&token	&feat name & feat value & Count\newline (illustrative) & Frequency\newline (illustrative)	\\ \midrule

% DET & buah & sebuah & Definite& Ind & 6 & 0.3\\
% DET & buah & sebuah & Number& unassigned & 1 & 0.05\\
% DET & buah & sebuah & PronType& Art & 10 & 0.5
% \\\midrule
% DET & para	& para	&Definite & unassigned & 1 & 0.1\\
% DET & para	& para	&Number & Plur & 4 & 0.4\\
% DET & para	& para	&PronType & Ind & 6 & 0.6
% \\\midrule
% ADJ&baru	&baru& Degree&unassigned & 4 & 0.8\\
% ADJ&baru	&baru& NumType&unassigned & 1 & 0.2 \\\midrule
% ADJ & akhir	&terakhir&	Degree& Sup & 3 & 0.25\\
% ADJ & akhir	&terakhir&	NumType & unassigned & 4 & 0.333\\
% \bottomrule
% \end{tabular}
% \end{table}


%%% SFM above moved to appendix %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first key difference between Çöltekin \& Rama's metric and ours is that we do not omit tokens that have no morphological features.
Instead, we introduce a dummy feature value `unassigned' for those tokens that do not have an assigned value for each morphological feature associated with that token's lemma.
%%% SFM word chopping
% For example, the determiner token `para' in the dataset described by Table \ref{tab:unassigned_ex} has no morphological feature corresponding to the category Definite.
% Supposing another token of the same lemma does have a feature value for Definite in this dataset, we would add the dummy feature `Definite=unassigned' to this token.
%%% SFM end word chopping
In this way we ensure that we include unmarked tokens in our calculation of surprisal.
This is important: if (say) 99\% of tokens of a given lemma have no Definite marking and 1\% of them have Definite=Ind, possession of a Definite marker should count as very surprising.
Without the dummy assignment this fact would be lost: the 1\% of Definite=Ind tokens would all get a frequency of 1 (because that's the only value for that feature across that lemma) and a surprisal of zero (because $\log{\frac{1}{1}}=\log{1}=0$).

A second key difference with Çöltekin \& Rama is as follows.
Instead of calculating the average surprisal of features across the entire sample of tokens, we split features into their component category/value pairs, and calculate the sum of the surprisals of feature values for a given token.
This produces what might be thought of as `the morpho-surprisal of the token': the amount of information a token carries by virtue of its morphological feature markings or lack thereof.
Once we have a measure of surprisal per token, we can average this across the entire dataset to obtain mean surprisal per token.
This indicates `the morpho-surprisal of the language': how surprising the morphology of tokens in this language tend to be.
If there are very many morphological features whose values are all somewhat evenly represented among the tokens of the dataset, mean surprisal will be high.
If on the other hand each lemma has only a few features, or if a small number of feature values are much more likely than others, mean surprisal will be small.

%%% SFM Chopping: moved to appendix
% Clearly our mean surprisal measure is capturing something similar to the classical entropy measure, but they are not strictly equivalent.
% Entropy weights surprisal by the same probability that defines the surprisal: the two $p$'s in the formula $\Sigma p\log{\frac{1}{p}}$ are the same.
% By contrast our measure calculates a sum of surprisals, each of which is defined in terms of the frequencies of its morphological feature values.
% Only then do we calculate a mean value by summing across the entire dataset.
% The formula describing our measure is therefore $\Sigma_i p_i \left( \Sigma_{j_i} \log{\frac{1}{p_{j_i}}} \right)$ where $i$ ranges across tokens and $j_i$ ranges across feature values of token $i$.
%%% SFM end chopping

There are several variants of this procedure, yielding slightly different measures of morphological information per dataset.
One variant aggregates features at the level of Part-Of-Speech (UPOS) rather than lemma.
In this case tokens receive many more ``dummy'' unassigned feature values: all those categories that occur on any token of the same UPOS, which will generally be a longer list than just those that occur on tokens of the same lemma.
Another variant controls for variability in morphological feature annotations across datasets by using only the ``core'' features as defined by the UD project (see appendix \ref*{caveat_comparability} for more).
A final variant treats the token's entire list of feature category-value pairs (after dummy value insertion) as the basis of the surprisal measure: only tokens with \textit{exactly the same} feature values get to count as `the same' for the purposes of calculating frequencies, hence surprisal.
We indicate this variant with the term `featstring', since the full list of features is concatenated into a single string before frequency calculation.

Overall, we define eight versions of our metric:

\begin{enumerate}
    \item \textbf{surprisal of morphological features per token}
    \begin{enumerate}
  \renewcommand{\labelenumi}{\alph{enumi})}
  \item \textbf{aggregation level}: lemma; \textbf{features:} all; \textbf{frequencies calculated over}: feature values
  \item lemma; core only; feature values
  \item UPOS; all; feature values
  \item UPOS; core only; feature values
  \item lemma; all; featstring
  \item lemma; core only; featstring
  \item UPOS; all; featstring
  \item UPOS; core only; featstring
\end{enumerate}
\end{enumerate}

We note caveats to our approach in appendix \ref*{sec:caveats}. 

\subsection{Additional metrics}
We also calculate related metrics which are commonly reported in other studies of morphological complexity in corpora or which can be useful in interpreting the custom metrics. For example, we expect that a dataset that contains more morphological feature categories are more likely to exhibit higher levels of surprisal of morphological features per token as there are more features that can be utilized. 
%Like for the metrics defined above, some of those listed here depend on whether the aggregation level is lemma or UPOS, and whether all features or only core features are considered.
\begin{enumerate}
  \setcounter{enumi}{1}  % This will start the list at 2
 %    \item Mean surprisal of morphological features when concatenated as a string, e.g. ``Number=Plur|PronType=Ind'' instead of split per feature and feature category versus feature value (see section \ref{caveat_conditional_order}). Labelled ``featstring'' in results plots. 
 %    \begin{enumerate}
 %    \renewcommand{\labelenumi}{\alph{enumi})}
 %  \item aggregation level = lemma, all features
 %  \item aggregation level = lemma, core features only
 %  \item aggregation level = UPOS, all features
 %  \item aggregation level = UPOS, core features only
 % \end{enumerate}
  \item Type-Token Ratio (TTR): the number of unique tokens divided by the total number of tokens
  \item Lemma-Token Ratio (LTR): the number of unique lemmas divided by the total number of tokens
  \item Mean number of morphological features per token (calculated without ``dummy'' features)
      \begin{enumerate}
  \renewcommand{\labelenumi}{\alph{enumi})}
  \item all features
  \item core features only
 \end{enumerate}
   \item Mean token surprisal: mean surprisal per token given all tokens, regardless of UPOS/lemma or morphological features
\end{enumerate}

\noindent We also report some simple statistics for each dataset, namely:

\begin{enumerate}
  \setcounter{enumi}{9}  % This will start the list at 5
  \item Number of types
  \item Number of tokens
  \item Number of unique feature categories (e.g. Tense, NumType)% (comparable to Grambank metrics Fusion and Informativity)
      \begin{enumerate}
    \renewcommand{\labelenumi}{\alph{enumi})}
 \item  all features
  \item core features only
 \end{enumerate}
 \end{enumerate}

We also compare to other metrics external to our study: 

\begin{itemize}
    \item Morphological feature entropy (MFH) as calculated by \citet{ccoltekin2023complexity} - with small modifications (see appendices \ref{sec:appendix_ccoltekin} and \ref{sec:appendixComparison})
\item Grambank metrics (for detailed definitions see \citet{grambank_release}, \citet{shcherbakova2023societies} and \citet{R-rgrambank}) 
    \begin{itemize}
    \item Fusion 
    \item Informativity 
    \end{itemize}
%Speaker population estimates, compiled by Google's research team \citep{ritchie-etal-2024-linguameta-unified}
\end{itemize}

\section{Results}
We report the results in ScatterPlOt Matrices (SPLOMs) and maps.\footnote{Unfortunately there are no UD datasets present in the Americas or Central or Eastern Pacific, which is why these regions are not displayed on the maps. Furthermore, the geographical points are tied to the locations of the languages in Glottolog 5.0 \citep{glottolog5.0}. The points have been jittered somewhat to avoid overlaps for dataset of the same/nearby languages which would obscure information.}
The SPLOM visualisations compare different measurements of the same data in a  pairwise fashion.
The lower-left portion of each SPLOM shows scatterplots for the pairwise comparisons of measurements.
The upper-right portion shows the Spearman's rank correlation coefficient of the corresponding pair. 
The diagonal of the plot shows histograms of each measure. 
The upper and right edges of the diagram list the names of each measure, while the left and lower edges display axis values for the scatterplots.

The Spearman correlation test evaluates the strength and significance of the relationship between the rank of two variables
The strength is expressed by a coefficient whose value lies between -1 (indicating negative correlation) and 1 (positive correlation). 
An absolute value of 1 indicates a perfect relationship in which the two variables are fully matched. 
Significance is measured via traditional p-values\footnote{For some of our variables, there were values that tied at the same rank. This makes it impossible to calculate  exact p-values for Spearman's rank correlation coefficient in the same way as when there are no ties. In such cases, we use an approximation that is well-established (asymptotic $t$ approximation) and implemented in the base R-package stat's function cor.test(method = "spearman", exact = FALSE).}.
Significant correlations (p<0.05) are presented in bold text and marked with an asterisk. 
An absolute correlation coefficient larger than 0.7 is commonly interpreted as a strong relationship, and above 0.9 very strong. 
We have marked significant correlations above 0.6 in red text.
For each pair of measurements, we compare as many overlapping data-points as possible.
The number of data-points compared for each pair is reported in the upper triangle of the SPLOM as n.\footnote{When comparing between measurements based on the UD datasets, each point represents a single UD dataset. When comparing against the Grambank metrics Fusion and Informativity and Google's population numbers, the UD datasets are matched to entries in Grambank via glottocodes \citep{glottolog5.0} using the documentation of each dataset as a guide. When comparing between the Grambank metrics and Google's population data, the comparison is only made for unique pairs of glottocodes in order to avoid double-counting. We only compare data-points for which at least one UD dataset exists.}
In addition to the raw language speaker population numbers from the Google data, we also include a logarithmic transformation.
This makes comparison to other metrics easier and reduces the effect of outliers \citep{changyong2014log}.
%%% SFM word chopping: added to appendix
% Since the Google data lists two languages as having zero population, and the logarithm of zero is undefined, we add 1 to all population figures before the transformation.
% This has the undesirable consequence of treating extinct languages as though they have a very small number of speakers.
% However, the relationship between population size and language features is likely coarse at best, so grouping extinct languages with very small languages does not jeopardise the broad conclusions that can be drawn.
%%% SFM end word chopping.

UD datasets can vary considerably in terms of genre, which can have an impact on the measurements we are calculating. 
The UD datasets that belong to the Parallel Universal Dependencies (PUD) subset are more comparable in genre. 
Therefore, we report results both for all UD datasets and for the PUD datasets only. 
See Appendix section \ref{sec:caveat_genres} for more information.

\subsection{Custom metrics}

First, we consider the relationship between the new metrics defined in this paper. 
Figures \ref{fig:SPLOM_metric_custom} and \ref{fig:SPLOM_metric_custom_PUD} show the relationship between the eight measurements we calculate, for all datasets and for PUD datasets only. 
% The measures are calculated separately for individual features and combinations of features (``featstring''), for aggregation levels UPOS and lemma, and with all features or just the core features.
% With three binary distinctions (feat vs featstring, UPOS vs lemma, core vs all features), we have eight different measures. 
Most of these correlate strongly with each other, suggesting that while they differ in specific implementation they are picking up on the same characteristics of the datasets (and thereby languages) in question.\footnote{A reviewer rightly pointed out that one cannot infer from a correlation between two variables that both are tracking the same underlying characteristic. For example, age and wage are correlated but do not track any single property in a population. Here we are relying on the fact that each of our metrics is defined in terms of the same formula, applied to the same datasets, but with variations in implementation. The relatively strong correlations between these implementations demonstrate, we think, that there is a general property of each dataset -- what we have described as the `morpho-surprisal' -- that each specific implementation is capturing an aspect of. It is as though we were to use several variants on the definition of `income' (gross wage, net wage, disposable income, etc.) and note that they are correlated because each captures some aspect of a more general property (`income' considered as an abstract kind, without specifying a precise formulation).} 
The correlations are even stronger if we consider only the PUD-subset of UD (Figure \ref{fig:SPLOM_metric_custom_PUD}), indicating that the variation among the full complement of datasets is partly due to genre.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex/graphics/SPLOM_metrics_custom.png}
    \caption{SPLOM of custom metrics (all UD datasets). Since all eight measures are variations on the theme `mean surprisal per token', the correlations are all strong or very strong. The third and sixth measures here are used for comparison with other metrics in Figures \ref{fig:SPLOM_other_metrics} and \ref{fig:SPLOM_metrics_external}.}
    \label{fig:SPLOM_metric_custom}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex/graphics/SPLOM_metrics_custom_PUD.png}
    \caption{SPLOM of custom metrics (PUD datasets only). Since the PUD datasets are smaller in size and uniform in genre, the correlations are stronger than their counterparts in Figure \ref{fig:SPLOM_metric_custom}. The third and sixth measures here are used for comparison with other metrics in Figures \ref{fig:SPLOM_other_metrics_PUD} and \ref{fig:SPLOM_metrics_external_PUD}.}
    \label{fig:SPLOM_metric_custom_PUD}
\end{figure}

\FloatBarrier
\noindent The two custom metrics that differ the most from each other are:
\begin{itemize}
    \item Mean sum surprisal of features, aggregation level = UPOS, all features (Figure \ref{fig:map_sum_surprisal_morph_split_mean_upos_all_features})
    \item Mean sum surprisal of featstring, aggregation level = lemma, core features only (Figure \ref{fig:map_surprisal_per_morph_featstring_mean_lemma_core_features_only})
\end{itemize}

Therefore, we will include these two in further plots as they represent the variation of our custom metrics. 
Their definitions are also complementary (features vs featstring, UPOS vs lemma, all features vs core only), so it makes sense that they would correlate the least with each other.
Figure \ref{fig:maps_custom_two} shows the distribution of these two metrics across the datasets linked to language locations. 
These distributions are similar to the morphological patterns of isolating//agglutinating/fusional languages often referenced in linguistic typology (cf. \cite{comrie1989language} and \cite{william1990typology}), with `hot spots' in eastern Europe and Central Asia and lower values in Southeast Asia. 
However, as several key areas are missing or severely under-represented (primarily the Americas and Australia), the seeming alignment of our values with patterns discussed in the literature is indicative rather than conclusive.


\begin{figure}[htbp]
  \centering
  \subfigure[Mean sum surprisal of features (in bits), aggregation level = UPOS, all features\label{fig:map_sum_surprisal_morph_split_mean_upos_all_features}]{
    \includegraphics[width=0.48\linewidth]{latex/graphics/map_sum_surprisal_morph_split_mean_upos_all_features.png}
  }
  \hfill
  \subfigure[Mean sum surprisal of featstring (in bits), aggregation level = lemma, core features only\label{fig:map_surprisal_per_morph_featstring_mean_lemma_core_features_only}]{
    \includegraphics[width=0.48\linewidth]{latex/graphics/map_surprisal_per_morph_featstring_mean_lemma_core_features_only.png}
  }
  \caption{Maps showing two of the custom metrics.}
  \label{fig:maps_custom_two}
\end{figure}

\subsection{Other UD metrics}

Second, we compare our custom metrics to other metrics and statistics calculable from UD datasets in Figures \ref{fig:SPLOM_other_metrics} (all datasets) and \ref{fig:SPLOM_other_metrics_PUD} (PUD datasets only).
In these figures, the two representatives from our set of custom metrics appear first, as the green and blue rows/columns at the upper left of the plot.
As expected, the \textbf{mean number of features per token} correlates with both of our custom metrics: more features entails a lower frequency per feature, which translates directly to higher surprisal.
We also see a strong correlation between our UPOS-level metric and the \textbf{total number of feature categories} in the datasets. 
The overall pattern is similar for both the all-datasets and PUD-subset comparisons.
One exception is the relationship between our UPOS-level metric and the type-token ratio (TTR), which is weak across all datasets but strong in the PUD subset. 
This suggests that genre has an impact on the variability of types in a dataset, underlining the need to use subsets such as PUD to ensure comparability.

\begin{figure}
    \centering
        \includegraphics[width=1\linewidth]{latex/graphics/SPLOM_metrics_other.png}  
    \caption{SPLOM additional metrics  (all UD datasets). The first two measures are representatives of our custom measures shown in Figure \ref{fig:SPLOM_metric_custom}, while the remaining measures are standard quantities calculated from the UD datasets.}
    \label{fig:SPLOM_other_metrics}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=1\linewidth]{latex/graphics/SPLOM_metrics_other_PUD.png}  
    \caption{SPLOM additional metrics (PUD datasets only). The first two measures are representatives of our custom measures shown in Figure \ref{fig:SPLOM_metric_custom_PUD}, while the remaining measures are standard quantities calculated from the PUD datasets. Number of sentences is not displayed as it is fixed for all PUD datasets (1,000 sentences).}
    \label{fig:SPLOM_other_metrics_PUD}
\end{figure}

\subsection{Comparison with mean feature entropy}
\citet{ccoltekin2023complexity} define a metric called mean feature entropy, in which all feature-value pairs in the entire dataset are aggregated without regard to lemma or part of speech.
A empirical comparison of the metric outcomes with our two representative metrics and TTR can be seen in Figure \ref{fig:SPLOM_metrics_external_CR}.
The correlations between our metrics and theirs are moderate to strong ( ($0.65^*-0.89^*$) or not significant.
The first notable point of comparison is that mean feature entropy assigns zero to some datasets that our metrics assign appreciable values to.
This is because Çöltekin \& Rama omit datasets with empty lemmas, and there are a couple of datasets with morphological information but without lemmas assigned (namely UD\_Faroese-FarPaHC.tsv and UD\_English-GUMReddit.tsv).
Secondly, our UPOS/all features metric assigns a wider range of values to the datasets, some of which are lower and some higher than mean feature entropy.
Third, both of our comparison metrics correlate more strongly with Grambank's Fusion metric, suggesting our measures better capture this aspect of a language's grammar (though see the next section).
For more information on the differences between our two approaches methodologically, see Appendix \ref{sec:appendixComparison}.

\begin{figure}
    \centering
  \centering
  \subfigure[All datasets \label{fig:SPLOM_metrics_external_CR}]{\includegraphics[width=0.48\linewidth]{latex/graphics/SPLOM_metrics_external_CR.png}
  }
  \hfill
  \subfigure[PUD datasets only \label{fig:SPLOM_metrics_external_CR_PUD}]{\includegraphics[width=0.48\linewidth]{latex/graphics/SPLOM_metrics_external_CR_PUD.png}
  }
  \caption{Comparison with \citet{ccoltekin2023complexity}'s metric}
    \label{fig:SPLOM_metrics_external_CR}
\end{figure}

\subsection{Grambank metrics}

Finally, we compare our custom metrics to the Grambank metrics Fusion and Informativity. 
The correlations between our metrics and the Grambank metrics are typically weak ($0.45^*-0.48^*$) or not significant (Figure \ref{fig:SPLOM_metrics_external_grambank_PUD}).
Only with the PUD datasets do we see strong correlations ($0.7^*$), between our two representative custom metrics and the Grambank Fusion measure, though these are generated by a relatively small sample of only 18 datasets.
This lack of correlation highlights the distinctiveness of corpus-based analyses of complexity: insofar as our range of metrics captures something real, it is something that shows up in feature frequencies, rather than the rules describing a language's grammar.
The facets of language complexity discussed earlier are sensitive not just to grammar but to usage, indicating the relevance of corpora for quantifying complexity.
We discuss these issues in more detail below.

\begin{figure}
    \centering
    \centering
  \centering
    \subfigure[All datasets \label{fig:SPLOM_metrics_external_grambank}]{\includegraphics[width=0.48\linewidth]{latex/graphics/SPLOM_metrics_external_Grambank.png}}
  \hfill
  \subfigure[PUD datasets only\label{fig:SPLOM_metrics_external_grambank_PUD}]{\includegraphics[width=0.48\linewidth]{latex/graphics/SPLOM_metrics_external_Grambank_PUD.png}}
    \caption{Comparison with Grambank metrics.}
    \label{fig:SPLOM_metrics_external_grambank_PUD}
\end{figure}

\FloatBarrier
\section{Discussion}
Overall, the different metrics we define in this paper tell a similar story. 
With one exception, they all strongly correlate with each other ($>=0.7^*$, see Figures  \ref{fig:SPLOM_metric_custom} and \ref{fig:SPLOM_metric_custom_PUD}). 
The difference between the full data and the PUD subset (Figures  \ref{fig:SPLOM_other_metrics} and \ref{fig:SPLOM_other_metrics_PUD}) suggests that genre has a considerable impact on the variation of types in the datasets. 
Corpus-based analyses should therefore take genre into account, and statistically control for it where possible.

The two custom metrics used for broader comparison in this study correlate either weakly ($0.45^*-0.48^*$) or strongly ($0.7^*$) with the Fusion metric derived from the Grambank dataset \citep{grambank_release, shcherbakova2023societies}, see Figures \ref{fig:SPLOM_metrics_external} and \ref{fig:SPLOM_metrics_external_PUD}. 
This suggests that they are capturing related phenomena. 
Our metrics and Fusion are also connected to the mean number of features per token: the more features in a dataset, the more scope each token has to increase its surprisal; likewise, having more features annotated in a corpus strongly increases a language's chance of being awarded a higher Fusion score based on its grammatical description.
We do not find the same pattern when comparing our custom metric to the Grambank Informativity metric. 
This may be due to differences in design choices concerning which grammatical domains to include in the two datasets (UD versus Grambank).
% This relationship can also be seen in the strong correlations between our custom metrics and the mean number of features per token. 
% The more morphological features a language has, the higher the Grambank Fusion score is and the more  

The comparisons of the custom metrics, additional metrics and Grambank metrics is interesting as it shows us that while they differ conceptually, for many pairs there is still a significant relationship.
The correlations with established metrics (TTR, Fusion etc.) also assures us that we are not entirely off the mark. 
Even though the approaches differ we would expect them to at least weakly correlate - which most of them do. 
However, this should not be taken to mean that it is irrelevant which metric of morphological information/complexity we choose. %(or combination of metrics, see \cite{ccoltekin2023complexity})'s dimensionality reduction proposal)
Which metric is most appropriate should be decided \textit{a priori} based on the conceptual underpinnings and theoretical support it has generally, and by reference to the specific research question.
As we are interested in the informational load of grammar and how language communities make use of it, the custom metrics defined in this paper align more with this aim than the other metrics discussed. 

The results show a weak negative correlation between most of the configurations of the two representative custom metrics and population size (see Figure \ref{fig:SPLOM_metrics_external}). 
This trend is not significant for the PUD dataset, which might be due to the smaller sample size (n=21) or the fact that controlling for genre negates the relationship.
A negative correlation between our metrics and population size is in keeping with theoretical expectations as laid out in section \ref{sec:background}. 
%Larger populations increase the probability of interacting with someone with whom you share little common ground, which in turn engenders a greater 
%Less shared common ground > more low-context communication > more information in linguistic code > more information in grammar.
However, it is necessary to investigate this with more sophisticated analyses than a Spearman correlation test.
For example, it is desirable to increase the sample size of comparable corpora and introduce controls for phylogenetic and spatial covariance \citep{shcherbakova2023societies}.

\section{Conclusion}
This paper had three aims:

\begin{enumerate}
    \item Develop a corpus-based metric of morphological information, as a proxy for one aspect of linguistic complexity
    \item Improve on existing measures by explicitly quantifying absent morphological features
    \item Provide a platform for future investigation into pragmatic constraints on grammar
\end{enumerate}

We find it is possible to utilise cross-linguistic corpora such as the UD datasets to explore morphological information in a sophisticated way with information-theoretic approaches. 
Our approach improves upon previous studies such as \citet{ccoltekin2023complexity} by taking into account tokens without morphological features when calculating the overall statistic for the entire dataset and stratifying surprisal by categories (UPOS or lemma). 
To address concerns of non-comparability due to design choices of specific UD dataset contributors, we compute our metrics both over all features annotated and only the core features which are centrally defined by the UD coordinators. 
To caution against non-comparability due to genre, we also present the results for all UD datasets and only those found in the PUD data. 
We find that our corpus-based measures of morphological surprisal typically correlate weakly or not at all with grammar-based measures of Fusion and Informativity (with stronger correlations on a restricted subset), signifying that they reveal other aspects of language complexity.
Grammatical descriptions specify possibilities and prohibitions; patterns in corpora reveal how this rule-set is utilised and made meaningful. 
The significant, if very weak, negative correlation between one of our metrics and population size suggests that this may be fruitful to explore further with more sophisticated statistical analysis.

%that primarily determine the difficulty of language learning, production, and comprehension.
% See appendix \ref{sec:caveat_genres} for more discussion of caveats.

% The availability of datasets such as UD \cite{UD_2.14}

%TC:ignore

\bibliographystyle{unified}
\bibliography{bib.bib, used_pkgs}

%\section{Acknowledgements}
%We are indebted to Matthew Spike for his input on this article in particular and for our discussions on ``complexity'' and information in general. We thank {\c{C}}a{\u{g}}r{\i} {\c{C}}{\"o}ltekin and Taraka Rama for generous their aid in understanding the intracies of their measurement. We also want to thank the audiences at the Working Group for Empirical Linguistics-seminar series (WoGEL) at the Department of Linguistics and Philology at Uppsala University, department seminars at the Department of Linguistic and Cultural Evolution (DLCE) at the Max Planck Institute for Evolutionary Anthropology and participants at the 2024 Biennial meeting of the Association for Linguistic Typology in Singapore. We have also benefitted from advice from Daan van Esch, code review by Christoph Rzymski and discussions with Angela Chira. We also acknowledge the aid from Natalie Korobzow in understanding Russian. 
%Finally, we'd like to thank the organisers of the workshop on Dependency Grammar for Typology at the ALT-meeting and editors of this special issue ( Andrew Dyer, Luigi Talamo, Annemarie Verkerk, Luca Brigada Villa, Erica Biagetti \& Diego Alves) as well as the other presenters at the workshop and the audience. 

\newpage


\setcounter{section}{0}
\renewcommand\thesection{\Alph{section}}
\section*{Appendices}

\section{Detailed procedures}
\subsection{Çöltekin \& Rama}\label{sec:appendix_ccoltekin}

Like us, Çöltekin \& Rama aim to capture how evenly spread are the morphological features of a language.
The canonical mathematical way to measure this `even-spreadedness' is entropy.
Defined as $\Sigma p \log{\frac{1}{p}}$ for probabilities $p$ of a given distribution, entropy is large when probabilities are more evenly distributed and small when a few probabilities are much greater than the others.
Thus entropy captures something like the average amount of uncertainty among a set of items.
Çöltekin \& Rama thus define a measure of the entropy of all morphological features in a dataset to capture one aspect of a language's complexity.
Their method of calculating this value is as follows:\footnote{This procedure describes the code which can be found at \url{https://github.com/coltekin/mcomplexity/blob/main/mlc-morph.py}, specifically the method \texttt{get\_mfh}.} %We are grateful to Çöltekin \& Rama for corresponding with us so that we could confirm our understanding of their procedure. Any remaining misunderstandings are our own.}:
\begin{enumerate}
\item Read in the UD-datasets such that multiword tokens are not included but empty-tokens are kept
\item For a given dataset, randomly sample sentences (with replacement) until you reach a fixed set of tokens (in the paper, they use 20,000). This process is repeated 100 times and the average entropy measures are reported
\item Remove tokens that are punctuation (UPOS=`PUNCT') or unanalysable tokens (UPOS=`X')
\item Remove tokens that have no morphological features
\item Remove tokens that have the UPOS NUM and where the token is non-alphabetical (i.e. ``50'')
\item Split the remaining tokens' morphological features into a list (e.g. Table \ref{tab:mfh} lists the individual features in the sample shown in Table \ref{tab:turkish_example})
\item Count how many times each feature occurs in the sample (column `Count' in Table \ref{tab:mfh})
\item Calculate the relative frequency of each feature (column `Frequency (p)' in Table \ref{tab:mfh}; this is the feature's Count value divided by the sum of the Count column for the entire sample)
\item Calculate entropy from the frequencies ($\Sigma p \log{\frac{1}{p}}$)
\item Run steps 2-7 multiple times for different samples of the same size and take the mean entropy.
\end{enumerate}

\noindent Table \ref{tab:mfh} shows the components of the entropy measure for the sample of tokens listed in Table \ref{tab:turkish_example}.
The total entropy for this sample is approximately 3.15 bits.
%In their study \citet{ccoltekin2023complexity} sample (with replacement) 20,000 tokens 100 times, generating an entropy measure each time and then taking the average.
This average is then an estimate of the `overall' entropy measure for the dataset.

\begin{table}[h]
    \centering
    \caption{Components of the measure of morphological feature entropy as defined by \citet{ccoltekin2023complexity} for the sample of tokens in Table \ref{tab:turkish_example}. The total feature count is 17, and each feature's Frequency is its Count divided by this total (rounded to three significant figures). The entropy of this sample is 3.15 bits (also to three significant figures).} %note table captions go above the table
    \label{tab:mfh}   
    \begin{tabular}{p{5cm}p{3cm}p{3cm}}
\toprule
	\textbf{Morphological feature}	&	\textbf{Count}	&	\textbf{Frequency (p)}	\\
    \midrule
	Case=Loc&1&0.0588       \\    \midrule
	Number=Sing&4&0.235    \\    \midrule
        Person=3&4&0.235		   \\    \midrule
	Case=Nom&1&0.0588	       \\    \midrule
	Case=Acc&1&0.0588		      \\    \midrule
        Aspect=Perf&1&0.0588      \\    \midrule
        Mood=Ind&1&0.0588		   \\    \midrule
        Polarity=Pos&1&0.0588		\\    \midrule
	Tense=Past&1&0.0588	     \\    \midrule
	VerbForm=Fin&1&0.0588	    \\    \midrule
	Voice=Cau&1&0.0588      	\\ \bottomrule

    \end{tabular}
\end{table}

\paragraph{Implementation in this project}
For comparison, we re-ran the code of \citet{ccoltekin2023complexity} on the same dataset as our analysis. 
To make the outcomes more comparable, we introduced a modification of their procedure. 
We did not do the bootstrapping sampling (sample sentences with replacement until you reach a given token target) - instead we ran the analysis on the exact same datasets as ours - i.e all tokens in the 173 UD datasets with more than 13,000 tokens (excluding tokens with UPOS PUNCT or X). 
We copied the python scripts graciously provide by Çağrı Çöltekin and Taraka Rama at \href{https://github.com/coltekin/mcomplexity/}{https://github.com/coltekin/mcomplexity/}, made a few small modifications and incorporated the modified scripts into our project's codebase. 
The modifications include removing the calling of sample\_nodes in get\_mfh, which had the consequence that some of the filtering in sample\_nodes happen in get\_mfh instead.

\subsection{The present study}\label{sec:appendix_ours}
%\subsubsection{Procedure for measuring mean surprisal per token}
The full procedure of our metric is as follows:
\begin{enumerate}
\item Collapse the files of each UD dataset (e.g. dev, test and train) to one table with one token per row (including multiword-tokens, component word tokens and empty node tokens)
\item Remove punctuation (UPOS=`PUNCT') and unanalysable tokens (UPOS=`X')\footnote{For more on UPOS X, please see \href{https://universaldependencies.org/u/pos/X.html}{https://universaldependencies.org/u/pos/X.html}}.\footnote{During development, we also tested filtering out tokens that are tagged as UPOS=`SYM' or that only consist of certain symbols such as \%. However, we found that these were not consistently treated across datasets, so excluding these tokens impaired comparability. We opted instead the same approach as \citet{ccoltekin2023complexity} and filtering out only the most obviously irrelevant tokens.}
\item Remove datasets with less than 13,000 tokens. 13,000 was chosen as a cut-off as this let us keep all PUD-datasets with more than 2 feature categories defined.
\item Remove empty nodes. UD allows the insertion of tokens that are not present in the surface sentence in order to facilitate certain kinds of syntactic analysis. For example, the insertion of ``likes'' between ``Bill'' and ``tea'' in the sentence ``Sue likes coffee and Bill tea''. We drop these tokens from our analysis as we want to analyse the surface form as closely as possible. 
%\footnote{For more details on empty nodes in UD, see \href{https://universaldependencies.org/format.html?utm_source=chatgpt.com#words-tokens-and-empty-nodes}{https://universaldependencies.org/format.html?utm_source=chatgpt.com#words-tokens-and-empty-nodes}}
\item Resolve multi-word tokens. Some datasets will split tokens which are taken as representing multiple words into separate tokens. For example contractions such as: ``don't'' in English could be split into ``do'' and ``n't''. For UD-datasets, words that belong to a multi-word token (e.g. ``do'' and ``n't'') can be included as separate tokens and marked for syntactic dependencies, part-of-speech (UPOS), lemma and morphological features. The multiword also appears as a token, but without UPOS, lemma or morphological features. In this project, we want to analyse the surface forms - therefore we retain the multiword and remove the component words. We assign the multiwork token the morphological features and upos of the component words (e.g. "AUX\_PART). This may also be beneficial for comparability as multi-words may represent highly grammaticalised items that may be treated differently across datasets (sometimes split, sometimes not). By not analysing them further, we may be avoiding issues of uneven level of granularity of analysis.\footnote{For more details on multi-word tokens in UD, see UD documentation: \href{https://universaldependencies.org/format.html\#words-tokens-and-empty-nodes}{https://universaldependencies.org/format.html\#words-tokens-and-empty-nodes}.}
\item Missing lemmas are replaced by the token.  There is variation in how datasets of UD treat certain tokens in terms of defining or not defining a lemma. For example, many proper nouns (PROPN) or adpositions (ADP) will lack a lemma annotation. Some datasets, like Arabic-NYUAD, leave the lemma field empty for most tokens where UPOS is ADP or PROPN. Conversely, in UD\_Greek-GUD all items of the same UPOS values have a defined lemma (majority of the time identical to the token). To normalise for our comparison, we replace missing lemmas with the content in the token field.
\item Define lemmas as a combination of lemma + UPOS, to ensure that lemmas that are spelled the same but belong to different UPOS can be reliably distinguished. \textit{Example:} the noun ``mark'' and the verb ``mark'' in English-PUD are both annotated as belonging to the lemma ``mark''. By combinig lemma with UPOS, we can distinguish these as ``mark\_NOUN'' and ``mark\_VERB''.
\item Determine the full set of morphological feature categories per lemma. \textit{Example:} the token `sebuah' in Table \ref{tab:unassigned_ex} has features Definite and PronType; supposing there were another token of the same lemma in the dataset that had a value for the category Number, then the full set of categories for `sebuah' would be Definite, Number and PronType.
\item Remove features that are not related to morphology: ``Abbr'' (abbreviations), ``Typo'' (whether or not a token has a typo) and ``Foreign'' (whether or not a token is deemed as in a foreign language)
\item Assign a dummy value to unassigned morphological categories for each token, given the other available feature categories for tokens of the same lemma. \textit{Example:} for `sebuah' we insert the dummy feature Number=unassigned.
\item Split the tokens' morphological features into a list (e.g. Table \ref{tab:unassigned_ex_SPLIT} lists the individual features for the sample shown in Table \ref{tab:unassigned_ex})\footnote{Some tokens have been given more than one feature value for the same feature category. This is allowable within the UD-framework, though the coordinators note that such multi-values should be used sparingly. For example, the adjective ``{\dolousfont ἀ}{\Timesfont όρατος}'' in the Ancient Greek PTNK-dataset is assigned ``Gender=Fem,Masc''. We are treating instances like these as a case of the feature value being ``Fem,Masc'' and not a case of both ``Gender=Fem'' and ``Gender=Masc''.}
\item Count how many times each feature \textit{value} occurs \textit{for that category, in tokens of that lemma} (column `Count' in Table \ref{tab:unassigned_ex_SPLIT})
\item Calculate the relative frequency of each feature value (column `Frequency' in Table \ref{tab:unassigned_ex_SPLIT}; this is the feature value's Count divided by the number of times that lemma appears in the dataset)
\item Calculate the surprisal of that feature value for that token: $\log{\frac{1}{\text{frequency}}}$
\item Calculate the total `surprisal of a token' by summing the surprisals of all its features (including dummy-assigned features)
\item Mean surprisal per token is the average of this total across all tokens in the dataset.
\item Remove datasets where there are 2 or fewer feature categories defined (in total) as these appear not representative of the languages.
\item If there are 2 or fewer unique types in the dataset, set TTR, LTR, surprisal per token and number of types to missing as these appear not representative of the languages. \footnote{There are four datasets included in our study that do not contain regular tokens. They are: Arabic-NYUAD, English-GUMReddit, Japanese-BCCWJ and
Japanese-BCCWJLUW. The tokens are annotated for dependencies and features, but the tokens themselves are missing or very irregular (types >=2). The lemma annotation is also irregular.} 

% \item Calculate the mean surprisal in the dataset by summing the surprisals and dividing by the number of tokens.
\end{enumerate}


Clearly our mean surprisal measure is capturing something similar to the classical entropy measure used by \citet{ccoltekin2023complexity}, but they are not strictly equivalent.
Entropy weights surprisal by the same probability that defines the surprisal: the two $p$'s in the formula $\Sigma p\log{\frac{1}{p}}$ are the same.
By contrast our measure calculates a sum of surprisals, each of which is defined in terms of the frequencies of its morphological feature values.
Only then do we calculate a mean value by summing across the entire dataset.
The formula describing our measure is therefore $\Sigma_i p_i \left( \Sigma_{j_i} \log{\frac{1}{p_{j_i}}} \right)$ where $i$ ranges across tokens and $j_i$ ranges across feature values of token $i$.

\begin{table}[h]
    \centering
    \caption{Four tokens from the treebank UD\_Indonesian-PUD. The token `sebuah' has no feature value for Number, but in this illustrative example we imagine that other tokens of the same lemma do have this feature. We therefore include it as an unassigned feature; likewise for `para' and the feature Definite. Of the adjectives, neither `baru' nor `terakhir' have a value for the feature NumType; we continue to imagine that there is at least one lemma for each of these tokens that does have a value for this feature, therefore both are given it as an unassigned feature. In fact the token `baru' has no feature values of its own; thus it takes all features possessed by any other token of the lemma `baru' as unassigned.} %note table captions go above the table
    \label{tab:unassigned_ex}   
    \begin{tabular}{p{1cm}p{1.4cm}p{1.5cm}p{3.5cm}p{2.5cm}}
\toprule

% UD\_Indonesian-PUD
%id	&
UPOS&lemma	&token	&feats & unassigned feats	\\ 
\midrule
DET & buah & sebuah 
& Definite=Ind|PronType=Art
& Number
\\\midrule
DET & para	& para	&Number=Plur|PronType=Ind & Definite
\\\midrule
ADJ&baru	&baru&
& Degree \newline
NumType 
\\\midrule
ADJ & akhir	&terakhir&	Degree=Sup& NumType\\\bottomrule
\end{tabular}
\end{table}

% OUR APPROACH, FEATURES SPLIT
\begin{table}[h]
    \centering
    \caption{Four tokens from the treebank UD\_Indonesian-PUD. For illustrative purposes we imagine this dataset contains 20 occurrences of the lemma `buah', 10 of `para', 5 of `baru' and 12 of `akhir'. The Count and Frequency columns, whose values in this table are also illustrative and not real, answer the question: `how often does this lemma have this value for this feature?'. The frequencies are used to calculate the surprisal of a particular token. For example, the surprisal of the token `sebuah' is $\log{\frac{1}{0.3}}+\log{\frac{1}{0.05}}+\log{\frac{1}{0.5}} = 7.06\text{ bits}$ (to three significant figures).} %note table captions go above the table
    \label{tab:unassigned_ex_SPLIT}   
    \begin{tabular}{p{0.7cm}p{1cm}p{1.4cm}p{1.3cm}p{1.5cm}p{1.4cm}p{1.6cm}}
\toprule

%id	&
UPOS&lemma	&token	&feat name & feat value & Count\newline (illustrative) & Frequency\newline (illustrative)	\\ \midrule

DET & buah & sebuah & Definite& Ind & 6 & 0.3\\
DET & buah & sebuah & Number& unassigned & 1 & 0.05\\
DET & buah & sebuah & PronType& Art & 10 & 0.5
\\\midrule
DET & para	& para	&Definite & unassigned & 1 & 0.1\\
DET & para	& para	&Number & Plur & 4 & 0.4\\
DET & para	& para	&PronType & Ind & 6 & 0.6
\\\midrule
ADJ&baru	&baru& Degree&unassigned & 4 & 0.8\\
ADJ&baru	&baru& NumType&unassigned & 1 & 0.2 \\\midrule
ADJ & akhir	&terakhir&	Degree& Sup & 3 & 0.25\\
ADJ & akhir	&terakhir&	NumType & unassigned & 4 & 0.333\\
\bottomrule
\end{tabular}
\end{table}

For comparing with the Grambank metrics Fusion and Informativity, match the UD-datasets to glottocodes and take the mean of our metrics.

To illustrate the general flow of actions and order of scripts, we have made a schematic illustration in Figure \ref{fig:data_processing_flowchart}.

\begin{figure}
    \centering
    \includegraphics[width=0.45\linewidth]{latex/graphics/UD_complexity data flowchart.png}
    \caption{General flowchart of data-processing.}
    \label{fig:data_processing_flowchart}
\end{figure}
\FloatBarrier
\paragraph{Applying approach to other datasets}
In this study, we specifically apply our approach to UD-datasets. However, any corpus that can be formatted into a table where every row is a token can be used as input. The specific function that calculates the different surprisal metrics is \texttt{calculate\_surprisal()} in code\textbackslash{}03\_process\_data\_per\_UD\_proj.R. The token-table needs to have the following columns (as exemplified in table \ref{tab:example_input}): dir (unique identifier of dataset), id (unique identifier of token in entire dataset), token, lemma, feats and upos.

\begin{table}[h]
    \centering
    \caption{Example of input-table to the function \texttt{calculate\_surprisal()}.} %note table captions go above the table
    \label{tab:example_input}   
    \begin{tabular}{p{1.55cm}p{1.7cm}p{1.3cm}p{2cm}p{3.2cm}p{1.5cm}}
\toprule
\textbf{dir}&  \textbf{id}	&	\textbf{token}	&	\textbf{lemma}	&	\textbf{feats}	&	\textbf{upos}	\\	\midrule
Ukranian-IU &  001\_0002\_1	&	\Timesfont{У}	&	\Timesfont{у}\_ADP	&	Case=Loc	&	ADP	\\	\midrule
Ukranian-IU & 001\_0002\_10	&	\Timesfont{зображення}	&	\Timesfont{зображення}\_NOUN	&	Animacy=Inan| Case=Nom| Gender=Neut|Number=Sing	&	NOUN	\\	\midrule
Ukranian-IU & 001\_0002\_11	&	\Timesfont{Венери}	&	\Timesfont{Венера}\_PROPN	&	Animacy=Anim| Case=Gen|Gender=Fem| NameType=Giv|Number=Sing	&	PROPN	\\	\midrule
Ukranian-IU & 001\_0002\_12	&	\Timesfont{та}	&	\Timesfont{та}\_CCONJ	&	unassigned=unassigned	&	CCONJ	\\	\midrule
Ukranian-IU & 001\_0002\_13	&	\Timesfont{Адоніса}	&	\Timesfont{Адоніс}\_PROPN	&	Animacy=Anim|Case=Gen| Gender=Masc| NameType=Giv|Number=Sing	&	PROPN	\\	\midrule
Ukranian-IU & 001\_0002\_2	&	\Timesfont{домі}	&	\Timesfont{дім}\_NOUN	&	Animacy=Inan|Case=Loc| Gender=Masc| Number=Sing	&	NOUN	\\	\midrule
Ukranian-IU & 001\_0002\_3	&	\Timesfont{римського}	&	\Timesfont{римський}\_ADJ	&	Case=Gen|Gender=Masc| Number=Sing	&	ADJ	\\	\midrule
Ukranian-IU & 001\_0002\_4	&	\Timesfont{патриція}	&	\Timesfont{патрицій}\_NOUN	&	Animacy=Anim|Case=Gen| Gender=Masc|Number=Sing	&	NOUN	\\	\midrule
Ukranian-IU & 001\_0002\_5	&	\Timesfont{Руфіна}	&	\Timesfont{Руфін}\_PROPN	&	Animacy=Anim|Case=Gen| Gender=Masc|NameType=Giv| Number=Sing	&	PROPN	\\	\midrule
Ukranian-IU & 001\_0002\_6	&	\Timesfont{була}	&	\Timesfont{бути}\_VERB	&	Aspect=Imp|Gender=Fem| Mood=Ind|Number=Sing| Person=unassigned|Tense=Past| VerbForm=Fin	&	VERB	\\	\midrule
Ukranian-IU & 001\_0002\_7	&	\Timesfont{прегарна}	&	\Timesfont{прегарний}\_ADJ	&	Case=Nom|Gender=Fem| Number=Sing	&	ADJ	\\	\midrule
Ukranian-IU & 001\_0002\_8	&	\Timesfont{фреска}	&	\Timesfont{фреска}\_NOUN	&	Animacy=Inan|Case=Nom| Gender=Fem|Number=Sing	&	NOUN	\\	\midrule
\bottomrule
    \end{tabular}
\end{table}

\subsection{Calculation of Grambank metrics}\label{sec:appendix_rgrambank}
The metrics Fusion and Informativity were calculated from Grambank v1.0 with the following procedure, using functions from the R-package rgrambank \citep{R-rgrambank}. 
As this package is not fully published at the time of submission of this manuscript, we have copied the relevant functions over into our project to allow for a robust user-experience.
We combine dialects into languages to make comparison matches possible.
If two dialects under the same language have different values for the same feature, we choose one value at random (using the function reduce\_ValueTable\_to\_unique\_glottocodes). 
For each metric, we remove languages with more than 25\% missing data across the features used to calculate that specific metric and then compute the metric (using make\_theo\_scores). 
This data wrangling procedure differs slightly from \citet{grambank_release} and \citet{shcherbakova2023societies} in terms of how the missing values are cropped (they instead crop the full dataset once and then calculate each metric), but the difference is very small. 
See section Analysis: PCA in \citet{grambank_release} for more details on the metrics.
%See example scripts accompanying the R-package rgrambank for illustration of procedure and differences.
Unlike \cite{shcherbakova2023societies}, but like \cite{grambank_release} we use the version of the Grambank metric Fusion which awards features that cover bound morphology \emph{and} other morphology half a Fusion-point. Features that relate to bound morphology have 1 fusion point. As in the other two papers, we disregard features which relate to free-standing marking (Fusion point = 0).

\FloatBarrier

\section{Comparison of C\&R and our metrics of feature entropy/suprisal}\label{sec:appendixComparison}

This section contains a detailed comparison between our metric and that of \citet{ccoltekin2023complexity} (C\&R).
In each case we believe our methodology is an improvement and we indicate exactly how.

\subsection{C\&R remove all tokens with empty lemmas and/or tokens; we don't}

C\&R remove tokens whose lemma is empty or whose token is empty.
However, some tokens have missing lemma because their lemma is understood to be identical to the token or for other reasons specific to the dataset (e.g. lemmatization efforts focussed on content words)  
For example, this is most likely the case in many instances in Arabic-NYUAD where the lemma field is empty for most tokens where the part-of-speech is adposition (ADP) or proper names (PROPN). 
Conversely, in Greek-GUD all items of the same UPOS values have a defined lemma (majority of the time identical to the token). 
We treat tokens with missing lemmas as genuine tokens. 
To normalise for comparison, we replace missing lemmas with the content in the token field.
There are also datasets with missing tokens, but where there is annotation for morphological features, such as Arabic-NYUAD. 
We believe these tokens still contain valuable information for the computation of morphological complexity and hence include them.
We do however exclude them from some analysis, such as TTR.
As a result, there are datasets for which C\&R's morphological feature entropy is calculated as zero but which have appreciable complexity by our metric. 
Examples of these are EXAMPLES, as can be seen in the results SPLOMS (REF TO FIGURES).

\subsection{C\&R remove numbers; we don't}

C\&R remove tokens whose UPOS is NUM and which are non-alphabetical.
This means they remove some tokens that are in fact tagged for morphological features. 

For example, in Russian-GSD we find the token ``30'' in the sentence ``Длительность инфузии составляет приблизительно 30 мин.'' (English translation: ``The duration of infusion is approximately 30 minutes''). 
The token ``30'' is tagged as having accusative case. 
We take that to mean that ``30'' is identical to ``тридцать'' (Cyrillic spelling of the cardinal numeral thirty in accusative form) and should be included when it comes to morphological features.
Another example can be found in Chinese-GSDSimp where there are tokens such as `50\%' whose morphological feature list includes `NumType=Card', contrasting with other NUM tokens with feature values `NumType=Ord'. 
We do not remove tokens of this kind in order to avoid artificially restricting the kinds of tokens whose morphological features are involved in the calculation of the metric. 

%One might ask: why not remove the numbers? 
%Perhaps NumType=Ord is not the kind of feature we want to include? 
%But that raises the question of which feature categories `genuinely' contribute to the kind of complexity we are estimating 
%In this project, we take a generous view and include all of the morphological features that UD contains (excluding Abbr, Typo and Foreign). and remove only tokens marked with parts of speech that clearly play no role in morphological complexity e.g. punctuation.
%We do segment results into ``all features'' and ``core features only'', for more detail see section \ref{sec_appendix:core_features}.

\subsection{C\&R sample; we use the entire dataset}

C\&R sample sentences (with replacement) until they reach 20,000 tokens, derive entropy and repeat this 100 times per dataset, yielding an average value for morphological feature entropy.
Their justification is that this aids comparability: two datasets, one with 50,000 tokens and the other with 1.5 million, would yield different orders of magnitude of data-points for analysis. 
To ensure all datasets provide the same amount of data and confidence intervals and standard deviations are comparable, bootstrapping sampling of this kind is often implemented.

Whether or not this justification is appropriate for the other metrics they calculate (e.g. Inflectional synthesis, Mean Size of Paradigm), it is not relevant for point estimates of morphological feature entropy which is the focus of this study.
Entropy is an average across an entire dataset, yielding a value whose units are surprisal per feature.
Sampling with replacement simply produces an approximation to the value that would be obtained by calculating the metric over the entire dataset.
This is true regardless of the dataset's size.

What is more, because the metric is an average, it remains comparable even across datasets, no matter the difference in their sizes.
So long as the dataset is representative of the language from which it is drawn, calculating the entropy across the whole dataset is the best estimate for the entropy of the language as a whole.
For these reasons, in our analysis we report mean feature entropy calculated over the entire dataset.

Of course, some datasets are too small too plausibly be representative.
That is why both we and C\&R exclude datasets with too few tokens from our analyses.


\subsection{C\&R ignore unmarked features; we include them}

We introduce a dummy value `unassigned' for tokens which (a) lack a particular morphological feature, and (b) belong to a category in which other tokens possess a determinate value for that feature.
This is important because the absence of a feature can carry information, namely that this token does not have this feature. 
As a result, on average our estimates of entropy would be higher than those of C\&R [though they aren't perfectly commensurate for other reasons.]

\subsection{C\&R group features across all tokens; we categorise by lemma/UPOS}

C\&R calculate the average surprisal of features across ALL tokens, whereas we group by (a) UPOS and (b) lemma.
Our grouping approach captures the rationale for operationalising speaker complexity in terms of informational measures: a speaker is not faced with the problem of determining, for some unspecified word, what its feature category/value pairs might be; rather, given a lemma representing the particular concept the speaker wants to express, they must determine which features are typically (or obligatorily) incorporated in whatever lexical form will express that lemma in the current context. 
As it were, instead of casting around the entire lexicon for the right inflection, the speaker has to `search' only the variants of the current lemma (or, in the UPOS condition, all lemmas of this part of speech).

\subsection{Evaluation}

One might think that points 1-2 here are arguments in favour of our method of preprocessing rather than our metric.
Only point 5 is about the metric proper, while point 3 concerns whether the metric is estimated or calculated in full, and 4 is a question of application or interpretation (i.e. how the metric should be applied to tokens without values for particular feature categories). 
Very well: we think that all five points count in favour of our overall approach, while point 5 specifically justifies using our metric instead of C\&R's mean feature entropy. 
We think this provides sufficient ground for the present study, especially given we are able to extend our analysis to TODO datasets compared to C\&R's TODO.

Furthermore, our metrics correlate more strongly with Grambank's Fusion measure (Figures \ref{fig:SPLOM_metrics_external} \& \ref{fig:SPLOM_metrics_external_PUD}).
Mean feature entropy correlates with Fusion too, but insofar as we are aiming to capture something within corpora that is also picked up by grammars of the languages in question, our metrics capture it more obviously.

\subsection{Consequences of the differences}\label{subsec:appendixConsequences}

The following examples refer to the part-of-speech versions of our metric, but the same points would hold for the lemma versions too.

First, it is possible for a dataset to receive a low mean feature entropy but a higher score from our metrics.
This is a consequence of the fact that we include dummy feature values on tokens for which other tokens of the same part of speech have been tagged.
Our rationale is that the lack of an inflectional signifier carries information: that this token \textit{does not have} this feature which other tokens of the same part of speech posses.
By adding dummy features we enable datasets to be assigned a greater score by our metric than by mean feature entropy.
To take an extreme example: suppose there were exactly two feature-value pairs across the whole dataset, which were annotated on exactly half of the tokens.
So every token has either values for both feature 1 and feature 2, or no value at all.
The mean feature entropy would be 1 bit, because there are two possibilities that occur equiprobably.
But we assign dummy features to the other half of the tokens, meaning that every token has one of two equiprobable feature values.
Each token thus has 2 bits of uncertainty (1 bit associated with the first feature and 1 bit with the second), and our metric would yield 2 bits.
An example of a dataset that receives a higher value on our metric than mean feature entropy is UD\_Latin-CIRCSE.
Our UPOS/core features metric assigns it 6.36 bits, while it has a mean feature entropy of 5.26.

Conversely, it is possible for a dataset to receive a \textit{lower} score on our metric.
Mean feature entropy registers the entropy across all features regardless of part of speech.
In our metric, surprisal values are based on frequencies determined with respect to all the other feature values in a token's part of speech.
As a result, a dataset with diverse features across different parts of speech will receive a high value of mean feature entropy; if those features are relatively homogenous \textit{within} each part of speech, they will receive a lower value from us.
For example, suppose a dataset had one feature with exactly eight values, with the values spread equiprobably across four parts of speech.
Each part of speech has the same number of tokens, and each has two feature values distributed evenly.
The mean feature entropy would be 3 bits, because there are eight equiprobable feature-value pairs and $\sum\frac{1}{8}\log{\frac{1}{\frac{1}{8}}}=3$.
But our metric would assign 1 bit per token, because within each part of speech there are two equiprobable values.
The average surprisal is therefore 1 bit, and our metric assigns 1 bit to the dataset where mean feature entropy gives 3 bits.
An example of this kind of situation is UD\_Afrikaans-AfriBooms.
Its mean feature entropy is 4.41 bits, yet our UPOS/core features metric gives a value of 1.26 bits.

While the ranking of Latin as more complex than Afrikaans is attested on both measures, our metric provides a much larger range of values within which to locate different datasets.
Figure \ref{fig:boxPlotsMetrics} depicts box plots of the values of all eight of our variants across all datasets in this study, together with the corresponding values for mean feature entropy.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex/graphics/box_plots_metrics.png}
    \caption{Box plots of the values of the various metrics used in this study. Datasets for which the mean feature entropy is zero have been omitted.}
    \label{fig:boxPlotsMetrics}
\end{figure}

\section{Caveats}
\label{sec:caveats}

\subsection{Comparability}
\label{caveat_comparability}
In comparative studies, it is desirable to compare like with like. There are several comparability challenges in this study. These are known and expected with datasets like UD \citep{UD_2.14}. We list the ones that are most important to our study here. 

\subsubsection{Variation in morphological annotation}\label{sec_appendix:core_features}
Datasets can vary in many aspects, one of them being the \textbf{level of analysis of morphology}. 
Some datasets may contain many different morphological features and some very few, despite the underlying languages being similar. 
This is due to the design choices of the contributors of each dataset, and variation of this kind is expected in a project like UD.
Unfortunately, incompatible levels of annotation or annotation schemes makes the datasets less comparable.
We address this by calculating our metrics once for all features specified in a given dataset and once for the ``universal'' morphological features only. 
The UD-documentation guidelines specify a subset of the morphological features as ``universal'' \citep{ud_2_feat_website}. 
The UD-documentation writes that these are described in a standardised manner, with some flexibility. 
However, ``[t]his does not mean that they occur in all languages. It means that they have been attested in more than one language and they are considered linguistically important. UD treebanks may use additional features and values if they are properly documented'' \citep{ud_2_feat_website}. 
This set of morphological features are defined centrally within the UD-project as a whole, as opposed to defined locally for each UD dataset. 
They are:\footnote{For more detailed definitions of morphological features in the UD datasets, see the UD website. We also note that three features originally included in this list -- Abbr (abbreviations), Typo and Foreign -- are excluded from all of our analyses as they do not correspond to aspects of morphological complexity.}
\begin{itemize}
    \item PronType: pronominal type 
    \item NumType: numeral type
    \item Poss: possessive
    \item Reflex: reflexive
    \item ExtPos: external part of speech
    \item Gender: gender
    \item Animacy: animacy
    \item NounClass: noun class
    \item Number: number
    \item Case: case
    \item Definite: definiteness or state
    \item Deixis: relative location encoded in demonstrative
    \item DeixisRef: person to which deixis is relative
    \item Degree: degree of comparison
    \item VerbForm: form of verb or deverbative
    \item Mood: mood 
    \item Tense: tense
    \item Aspect: aspect 
    \item Voice: voice
    \item Evident: evidentiality
    \item Polarity: polarity
    \item Person: person
    \item Polite: politeness
    \item Clusivity: clusivity
\end{itemize}

As core features are more standardised across UD datasets, albeit with some flexibility, we expect these features to be the most comparable across datasets.\footnote{For an example of this flexibility, the UD guidelines note that while gender is commonly associated with nouns, there is nothing in the central guidelines that prevent the feature from also appearing on adjectives and verbs.}
We use the term ``core'' in this paper for this class of morphological features so as to not confuse it with other senses of the term ``universal''.

\subsubsection{Variation in Universal Part-Of-Speech annotation (UPOS)}
The UD datasets use a specific set of Universal Part-Of-Speech categories (UPOS):

\begin{itemize}
    \item ADJ: adjective
    \item ADP: adposition
    \item ADV: adverb
    \item AUX: auxiliary
    \item CCONJ: coordinating conjunction
    \item DET: determiner
    \item INTJ: interjection
    \item NOUN: noun
    \item NUM: numeral
    \item PART: particle
    \item PRON: pronoun
    \item PROPN: proper noun
    \item PUNCT: punctuation
    \item SCONJ: subordinating conjunction
    \item SYM: symbol
    \item VERB: verb
    \item X: other
\end{itemize}

These are defined centrally across datasets, but as with morphological features there is also language specific variation in how they are defined and applied. 
Parts-Of-Speech/word classes famously vary across languages in terms of which ones exist, how they behave, how they subdivide further, and so on. 
Some languages do not have an identifiable class of adjectives, with such meanings instead being expressed with nouns or verbs. 
Some languages have a fluid boundary between nouns and verbs. 
There is a substantial literature in linguistic theory and typology that deals with the definition of parts-of-speech/word classes across languages (c.f. \citet{kira_keira}).
For this project, we are naturally dependent on how the UD-central team have defined these labels and how the contributors of each dataset have then further specified them.

For example, in the UD dataset Abaza-ATB (glottocode = abaz1241, ISO 639-3 abq) there are tokens which are marked as having UPOS VERB but which have very different morphological features. 
Consider the token {\Timesfont цIитI} in Table \ref{tab:Abaza_sent_10} and {\Timesfont йазкIкIыта} in Table \ref{tab:Abaza_sent_11} which are both marked as UPOS VERB. 
The first token, {\Timesfont цIитI}, is annotated for features often associated with finite verbs such as tense. 
The token {\Timesfont йазкIкIыта} is likewise also tagged as UPOS VERB, but the morphological features are quite different. 
The morphological features of {\Timesfont йазкIкIыта} are all related to the gender, person and number of the arguments of the verb (io = indirect object, abs = absolutive). 
The two tokens do not share any features: {\Timesfont цIитI} is not marked for gender/person/number of the arguments and {\Timesfont йазкIкIыта} is not marked for tense or VerbForm.
Based on studying these annotations alone, we may reach the conclusion that there are at least two categories of tokens which are both included under the heading VERB and that have very different morphological behaviour.
This is not unexpected and is likely to occur for many datasets in UD, to different degrees.

\begin{table}[]
    \centering
    \begin{tabular}{p{3cm}|p{8cm}}
    Abaza     & {\Timesfont{щарда \underline{цIитI} ари агIаншижьтара}} \\
    Russian    & {\Timesfont{С тех пор прошло много времени }}\\
    English & A lot of time has passed since then\\
    Morphological features & Tense=Pres|VerbForm=Fin \\
    \end{tabular}
    \caption{Sentence 10 from UD dataset Abaza-ATB (UD v2.14). Only the morphological features for the underlined word are listed.}
    \label{tab:Abaza_sent_10}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{p{3cm}|p{8cm}}
    Abaza     & {\Timesfont{апхъа азаман уагIахъата адунай йыквыз шабгаз \underline{йазкIкIыта} зджьара йбзазун }}\\
    Russian    &{\Timesfont{В то время люди, которые находились на свете, собравшись, жили где-то }}\\
    English & At that time, the people who were in the world, having gathered, lived somewhere\\
    Morphological features & Gender[io]=Neut|Number[abs]=Plur|Number[io]=Sing| Person[abs]=3|Person[io]=3 \\
    \end{tabular}
    \caption{Sentence 11 from UD dataset Abaza-ATB (UD v2.14). Only the morphological features for the underlined word are listed.}
    \label{tab:Abaza_sent_11}
\end{table}

The UD annotation guidelines states that a verb ``is a member of the syntactic class of words that typically signal events and actions, can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in the clause'' \citep{ud_2_VERB_website}. 
They go on to note that the UPOS-label VERB is not applied to auxiliary verbs. 
The documentation further specifies that, depending on language-specific analysis, (i) participles may be tagged as VERB or ADJ, (ii) gerunds and infinitives may be analysed as VERB or NOUN and that (iii) converbs (transgressives) or adverbial participles may be classed as VERB or ADV.
This standardised definition therefore still leaves a lot of flexibility, as is desirable and expected in a project of this kind.

Given the structure of the language at hand and the specific decisions of the contributors, this can mean that the Parts-Of-Speech-topology can differ a lot between datasets. 
For the study at hand, this means that when the imputation of dummy features depends on the annotation of other tokens of the same UPOS, the Abaza tokens above would receive a lot of dummy features. 
{\Timesfont цIитI} (Table \ref{tab:Abaza_sent_10}) will receive the features Gender[io]=unassigned, Number[abs]=unassigned, and so on.
This is one of the reasons why we compute the metrics with the aggregation level set to both UPOS and lemma.

\subsubsection{Variation in genre}
\label{sec:caveat_genres}
Another comparability issue is related to \textbf{genre}, as the datasets of UD can be of very different genres. 
For example, Beja-NSC is based on linguistic fieldworker Martine Vanhove's corpus which features transcribed spoken stories, whereas the UD dataset Czech-Poetry contains, as the name suggests, samples of Czech 19th-century poetry. 
Ukrainian-ID consists of, as many datasets in UD do, a blend of fiction, news, opinion pieces, Wikipedia articles, legal documents, letters, posts and comments. 
We may expect that genre affects the structure and lexicon of the datasets, with poems exhibiting more experimental word order and legal documents more technical jargon. 
Figure \ref{fig:genre_doughnut} shows a frequency doughnut-plot of genre tags for the 158 datasets included in this study.
``News'' is the most common genre tag (n=102), with nonfiction coming second (n=66).

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{latex/graphics/genre_doughnut.png}
    \caption{Doughnut plot of frequency of genres in the dataset}
    \label{fig:genre_doughnut}
\end{figure}

While we present the metrics as they are calculated over each dataset in UD, future studies could delve more into the differences between datasets as they relate to the genre of the content.
Some of the UD datasets are more comparable in genre than others, in particular those that stem from the \hyperlink{http://universaldependencies.org/conll17/}{Conference on Computational Natural Language Learning 2017 shared task on Multilingual Parsing from Raw Text to Universal Dependencies}.
These datasets are all based on the same texts from news and Wikipedia, translated into different languages.
These datasets are referred to as the Parallel Universal Dependencies (PUD) treebanks.
We separate out analysis for PUD datasets only as they represent highly comparable data in terms of genre.

Each PUD dataset is described the same way in the accompanying documentation \citep{UD_2.14}:
\begin{quote}
    
\textit{
There are 1000 sentences in each language, always in the same order. (The sentence alignment is 1-1 but occasionally a sentence-level segment actually consists of two real sentences.) The sentences are taken from the news domain (sentence id starts in ‘n’) and from Wikipedia (sentence id starts with ‘w’). There are usually only a few sentences from each document, selected randomly, not necessarily adjacent. The digits on the second and third position in the sentence ids encode the original language of the sentence. The first 750 sentences are originally English (01). The remaining 250 sentences are originally German (02), French (03), Italian (04) or Spanish (05) and they were translated to other languages via English. Translation into German, French, Italian, Spanish, Arabic, Hindi, Chinese, Indonesian, Japanese, Korean, Portuguese, Russian, Thai and Turkish has been provided by DFKI and performed (except for German) by professional translators. Then the data has been annotated morphologically and syntactically by Google according to Google universal annotation guidelines; finally, it has been converted by members of the UD community to UD v2 guidelines.}

\textit{
Additional languages have been provided (both translation and native UD v2 annotation) by other teams: Czech by Charles University, Finnish by University of Turku and Swedish by Uppsala University.}

\end{quote}

%\paragraph{Phonological realisation of morphological features}

\subsection{Additional informational facets of morphological features}
\label{caveat_conditional_order}
Morphological features are liable to support probabilistic dependencies.
For example, it is possible that if the morphological feature category Case takes the value Nominative, then the category Animacy is more likely to take the value Animate.
Our analysis ignores these dependencies for the most part (though some of them could be tracked implicitly by the `featstring' measures).
This is suboptimal as the informational load of a token would be reduced by probabilistic relations between its morphological components.
We are therefore typically over-estimating the surprisal of a token, as most of our measures treat morphological features as probabilistically independent of each other.
It would be possible to define a more nuanced measure that takes these conditional dependencies into account.

%Ignoring order of elements

%\subsection{Population figures}
%Since the Google data lists two languages as having zero population, and the logarithm of zero is undefined, we add 1 to all population figures before the logarithmic transformation.
%This has the undesirable consequence of treating dormant languages with no listed speakers as though they have a very small number of speakers.
%However, the relationship between population size and language features is likely coarse at best, so grouping dormant languages with very small languages does not jeopardise the broad conclusions that can be drawn.

\section{Data and code availability}
All code in this project is available freely at this anonymised OSF-project: \url{https://osf.io/xmb6w/?view_only=6dc1e1caffa64b4a98b3a40591c6891d}.

All data in this study is available freely, see the README in the code project for details versions and access.

All the analysis for this research project was done in the free and open source programming language R and Python, using a multitude of packages. 

We gratefully acknowledge and cite the packages used: \input{citation_keys.txt}.

Table \ref{r_package_table} shows all R-packages used and their versions. The list of R-packages is also found in the code project, accompanied by scripts and files to install and load the appropriate versions for MacOS and Windows-users.

\input{used_packages_table.tex}


%TC:endignore
\end{document}